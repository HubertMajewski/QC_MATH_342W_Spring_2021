---
title: "Practice Lecture 17 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 7, 2021"
---

# Logistic Regression for Binary Response

Let's clean up and load the cancer dataset, remove missing data, remove the ID column and add more appropriate feature names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've done this awhile ago!!!
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
head(biopsy$class)
```

We can either estimate probability of the biopsy tissue being benign (this would mean y = 1 is the benign category level) or estimate the probability of the biopsy tissue being malignant (this would mean y = 1 is the malignant category level).

Let's go with the latter. To make the encoding explicitly 0/1, we can cast the factor to numeric or we can rely on R's default factor representation i.e. that the first level is 0 and the second level is 1. Here, we can use this default without reordering since the levels above show that benign is first and thus  = 0 and malignant is second and thus = 1 (via coincidence of alphabetical order).

Now let's split into training and test for experiments:

```{r}
set.seed(1984)
K = 5
test_prop = 1 / K
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```


Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to set the family parameter to be "binomial" which means we are using the independent Bernoulli and within the binomial family, we are using the "logit" link. There are other types of family models we won't get a chance to study e.g. Poisson, negative binomial for count models

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = binomial(link = "logit"))
```

That was fast! There was actually a lot of optimization in that line. Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit the log odds of malignancy increases by 0.597...

All of the coefficients are positive which means if any of the covariates increase...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malginant. If you can read log odds, you'll see ... has a small change of being malignant and ... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

It's very sure of itself! 

Let's see $y$ by $\hat{p}$ another way:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there in the training set! How about the test set?

```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = y_test), alpha = 0.5)
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Looks pretty good! 

We now will talk about error metrics for probabilistic estimation models. That will give us a way to validate this model and provide an estimate of future performance. 

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Yup can't do arithmetic operations on factors. So now we have to go ahead and cast.

```{r}
y_train_binary = ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_binary - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on. And the oos Brier score?

```{r}
y_test_binary = ifelse(y_test == "malignant", 1, 0)
mean(-(y_test_binary - p_hats_test)^2)
```

Not as good but still very good!

What is the in-sample log score?

```{r}
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
```

This isn't bad (if you get intuition in reading them). And oos?

```{r}
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Not as good but still very good!

If we wanted to be more careful, we can use K-fold CV to get a less variable oos metric. Maybe we'll do that in a lab?


# Probit and Cloglog probability estimation

These are different generalized linear models but fit using the same code. All we need to do is change the link argument. For a probit regression we just do:

```{r}
probit_mod = glm(class ~ ., biopsy_train, family = binomial(link = "probit"))
```

This is complaining about numerical underflow or overflow. If you get a z-score that's really large in magnitude, then it says probability is 1 (if z score is positive) or 0 (if z score is negative)

```{r}
coef(probit_mod)
```

As we saw before, all coefficients for the covariates are positive. What's the interpretation of b for bare_nuclei?

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(probit_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

This is basically the same. How about out of sample?


```{r}
p_hats_test = predict(probit_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Also basically the same-looking. To get an apples-apples comparison with logistic regression let's calculate the brier and log scoring metrics:

```{r}
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

It appears the logistic regression is better oos.

Let's do complementary log-log too:

```{r}
cloglog_mod = glm(class ~ ., biopsy_train, family = binomial(link = "cloglog"))
coef(cloglog_mod)
```

Same signs on coefficients. Interpretation? Difficult... 

Let's see how it does compared to the logistic and probit models.

```{r}
p_hats_train = predict(cloglog_mod, biopsy_train, type = "response")
p_hats_test = predict(cloglog_mod, biopsy_test, type = "response")
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Much worse than either! 

Logistic regression is usually the default. But just because it's the default and most popular and just because it won here doesn't mean it will always win!! Using probit or any other link function constitutes a completely different model. You can use the "model selection procedure" we discussed before to pick the best one.


```{r}
rm(list = ls())
```

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
adult$fnlwgt = NULL
adult$occupation = NULL
adult$native_country = NULL
```

Let's use samples of 5,000 to run experiments:

```{r}
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial") #shortcut for binomial(link = "logit")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
length(coef(logistic_mod))
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)), alpha = 0.5)
```

Much more humble!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and usually confident about the small incomes.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
table(as.numeric(y_train)) #casting works... almost...
y_train_numeric = as.numeric(y_train) - 1 #or ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is a decent Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyon understands classification errors!

