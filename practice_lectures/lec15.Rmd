---
title: "Practice Lecture 15 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "March 22, 2021"
---

## For MA students only: Eigendecomposition in R and the Projection Matrix

```{r}
B = array(seq(1, 4), dim = c(2, 2))
B
eigen_decomp = eigen(B)
V = eigen_decomp$vectors
V
v1 = V[, 1, drop = FALSE]
v2 = V[, 2, drop = FALSE]
lambdas = eigen_decomp$values
lambdas
lambda1 = lambdas[1]
lambda2 = lambdas[2]
B %*% v1
lambda1 * v1
B %*% v2 
lambda2 * v2
B %*% v2 == lambda2 * v2 #why not?
#diagonolization
V %*% diag(lambdas) %*% solve(V)
```

Let's take a look at a projection matrix:

```{r}
X = model.matrix(medv ~ ., MASS::Boston)
head(X)
H = X %*% solve(t(X) %*% X) %*% t(X)
H[1:5,1:5]
dim(H)
eigen_decomp = eigen(H)
lambdas = eigen_decomp$values
head(lambdas, 50)
lambdas = as.numeric(eigen_decomp$values) #coerce to real numbers (good enough approximation)
head(lambdas, 50)
lambdas_rounded = round(lambdas, 10)
head(lambdas_rounded, 50)
table(lambdas_rounded)
sum(lambdas_rounded) #i.e. the rank(H) = # cols in X
```

It turns out all eigenvalues of a projection matrix are either 0 or 1. Why is this? It makes sense since:

H 1-vec = 1-vec
H x_{dot 1} = x_{dot 1}
H x_{dot 2} = x_{dot 2}
.
.
.
H x_{dot p} = x_{dot p}

All these p+1 eigenvalues of H are 1 and there are p+1 of them. Further,

```{r}
sum(diag(H))
```

The trace (sum of the diagonal entries) is also = p+1 i.e. the rank[X]. This is true since the trace of a matrix is always equal to the sum of the eigenvalues since tr(H) = tr(V^-1 D V) = tr(V^-1 V D) = tr(D) = sum lambda_i which in this case is rank[X].

What are the eigenvectors? Only the first ncol(X) = 14 reconstruct the space. Are they the same as the X columns?

```{r}
V = eigen_decomp$vectors
V = matrix(as.numeric(V), ncol = ncol(V))
dim(V)
V[1 : 5, 1 : ncol(X)]

V_X_angles = matrix(NA, ncol(X), ncol(X))
for (i in 1 : ncol(X)){
  for (j in 1 : ncol(X)){
    if (j > i){
      V_X_angles[i, j] = acos(V[, i] %*% X[, j] / sqrt(sum(V[, i]^2) * sum(X[, j]^2))) * 180 / pi
    }
  }
}
V_X_angles
```

No... and they don't need to be. They just need to represent the same space i.e. their columns need to span the same 14-dimensional subspace. You can check this by just comparing the orthogonal projection matrix which is unique for each subspace.

```{r}
V_X = V[, 1 : ncol(X)]
H_V = V_X %*% solve(t(V_X) %*% V_X) %*% t(V_X)
dim(H_V)
testthat::expect_equal(c(H_V), c(H))
```

And they're the same as expected.

# The rest of the Midterm Review Lab

Run everything relevant from before:

```{r}
pacman::p_load(ggplot2)
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
diamonds = diamonds[diamonds$carat <= 2 & diamonds$x != 0 & diamonds$y != 0 & diamonds$z != 0 & diamonds$depth != 0 & diamonds$table != 0,]
diamonds$ln_price = log(diamonds$price)
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
n = nrow(diamonds)
set.seed(1984)
diamonds = diamonds[sample(1 : n), ]
```


Note: I will now model price, not ln_price as ln_price yields residuals that are orders of magnitude. Fitting a good ln_price model will take more time.

```{r}
#All model formulas for reuse later
model_formulas = list(
  A = price ~ ln_carat,
  B = price ~ ln_carat * clarity,
  C = price ~ ln_carat * (clarity + cut + color),
  D = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut + color),
  E = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + carat + x + y + z + depth + table) * (clarity + cut + color)
)
#Model A
mod = lm(model_formulas[["A"]], diamonds)
summary(mod)$sigma
mod$rank
#Model B
mod = lm(model_formulas[["B"]], diamonds)
summary(mod)$sigma
mod$rank
#Model C
mod = lm(model_formulas[["C"]], diamonds)
summary(mod)$sigma
mod$rank
#Model D
mod = lm(model_formulas[["D"]], diamonds)
summary(mod)$sigma
mod$rank
#Model E 
mod = lm(model_formulas[["E"]], diamonds)
summary(mod)$sigma
mod$rank
```

Big win on E... the reason I think is that now we have a flexible curve for each continuous feature and that curve changes with all the categorical features.

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!


```{r}
#Model F
model_formulas[["F"]] = price ~ 
        (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + poly(carat, 3) + poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * (cut + color + clarity)
mod = lm(model_formulas[["F"]], diamonds)
summary(mod)$sigma
mod$rank
```

Can you think of any other way to expand the candidate set curlyH? Discuss.

We now have a very flexible curve for each continuous feature and that curve is fit individually for each level of all categories. We could indeed go further by allowing the levels among cut, color and clarity to interact and further allowing the continous features to interact. 

There is another strategy to increase the complexity of H without using function transforms of the features and interactions... we will have to see after winter break...

We should probably assess oos performance now. Sample 4,000 diamonds and use these to create a training set of 3,600 random diamonds and a test set of 400 random diamonds. Define K and do this splitting:

```{r}
K = 10
n_sub = 4000
set.seed(1984)
n_test = 1 / K * n_sub
n_train = n_sub - n_test
test_indicies = sample(1 : n, n_test)
train_indicies = sample(setdiff(1 : n, test_indicies), n_train)
all_other_indicies = setdiff(1 : n, c(test_indicies, train_indicies))
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 3600.

```{r}
oos_se = list()
all_models_train = list()
for (model_idx in LETTERS[1 : 6]){
  all_models_train[[model_idx]] = lm(model_formulas[[model_idx]], diamonds[train_indicies, ])
  summary(all_models_train[[model_idx]])$sigma
  oos_se[[model_idx]] = sd(diamonds$price[test_indicies] - predict(all_models_train[[model_idx]], diamonds[test_indicies, ]))
}
oos_se
```

You computed oos metrics only on n_* = 400 diamonds. What problem(s) do you expect in these oos metrics?

They are variable. And something is wrong with F! Possibly Runge's phenomenon?

To do the K-fold cross validation we need to get the splits right and crossing is hard. I've developed code for this already. Run this code.

```{r}
set.seed(1984)
temp = rnorm(n_sub)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
head(folds_vec, 100)
```

Comment on what it does and how to use it to do a K-fold CV:

This code tells you which index is inside of which fold's test set.

Do the K-fold cross validation for all models and compute the overall s_e and s_s_e. 

```{r}
oos_se = list()
oos_s_se = list()
for (model_idx in LETTERS[1 : 6]){
  e_vec_k = list() #for each one
  for (k in 1 : K){
    test_indicies_k = which(folds_vec == k)
    train_indicies_k = which(folds_vec != k)
    mod = lm(model_formulas[[model_idx]], diamonds[train_indicies_k, ])
    e_vec_k[[k]] = sd(diamonds$price[test_indicies_k] - predict(mod, diamonds[test_indicies_k, ]))
  }
  oos_se[[model_idx]] = mean(unlist(e_vec_k)) #note: not exactly the overall sd, but close enough
  oos_s_se[[model_idx]] = sd(unlist(e_vec_k))
}
res = rbind(unlist(oos_se), unlist(oos_s_se))
rownames(res) = c("avg", "sd")
res
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

Yes because it is an average so its standard error is approximately s_e / sqrt(K). It's only approximate since this is the formula for the std err or independent samples and the K oos samples are not exactly independent.

Model F seems more varialbe than all of them. Why? Possibly Runge's phenomenon?

Imagine using the entire rest of the dataset besides the 4,000 training observations divvied up into slices of 400. Measure the oos error for each slice and also plot it.

```{r}
n_step = 1 / K * n_sub
oos_se = list()
ses = list()
starting_ks = seq(from = 1, to = (length(all_other_indicies) - n_step), by = n_step)
for (model_idx in LETTERS[1 : 6]){
  se_k = list() #for each one
  for (k in 1 : length(starting_ks)){
    diamonds_k = diamonds[all_other_indicies[starting_ks[k] : (starting_ks[k] + n_step - 1)], ]
    se_k[[k]] = sd(diamonds_k$price - predict(all_models_train[[model_idx]], diamonds_k))
  }
  ses[[model_idx]] = se_k
  oos_se[[model_idx]] = unlist(se_k)
}

pacman::p_load(reshape2)
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model")
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model") + ylim(0, 5000)
```

What do we learn from this? Model F is very risky. What exactly is going wrong? Part of the agony of data science is the "debugging" just like when debugging software. But here the errors can be in the modeling too! 

```{r}
max(unlist(ses[["F"]]))
k_BAD = which.max(unlist(ses[["F"]]))

diamonds_BAD = diamonds[all_other_indicies[starting_ks[k_BAD] : (starting_ks[k_BAD] + n_step - 1)], ]
diamonds_BAD

e_BAD = diamonds_BAD$price - predict(all_models_train[[model_idx]], diamonds_BAD)
tail(sort(abs(e_BAD)))
diamonds_BAD[which.max(abs(e_BAD)), ]



summary(diamonds[train_indicies, ])


```

Is it extrapolation? Yes... y = 58.9 and in a cubic function, that would be astronomical. The real mistake is that y = 58.9 is impossible. The data wasn't cleaned. We will have an exercise in that. But this happens all the time and you don't want to be using polynomial functions for this reason since the new data may extrapolate very poorly.

Luckily we won't be dealing with these much longer since there are less risky ways of creating a transformed basis of Xraw.


# Popular Add-on Packages to R

## MLR library for CV

"Machine Learning in R" (the `mlr3` package v0.11) is a very popular R library that makes it very simple to build models, do validation, etc. This package was recently rewritten (September, 2019).

```{r}
pacman::p_load(ggplot2, mlr3verse)
```

It splits the modeling task into conceptual pieces. The most basic pieces are:

* Instantiate a "task". This consists of supplying a dataframe, identifying a variable that is the output and the type of predictions to be made.
* Instantiate a "learner". This consists of $\mathcal{A}$ and $\mathcal{H}$. For example: OLS with all raw features.
* Instantiate a type of validation. For example: 5-fold CV resampling.
* Execute

Here's what this would look like for our example we just did:

```{r}
modeling_task = TaskRegr$new(id = "diamonds", backend = diamonds, target = "price") #instantiate the task
algorithm = lrn("regr.lm") #instantiate the OLS learner algorithm on the diamonds dataset and set y = price
validation = rsmp("cv", folds = 5) #instantiate the 5-fold CV
resample(modeling_task, algorithm, validation) #execute
```

It's squabbling about something insignificant but at least it tells me exactly what I need to do to fix! Let's correct this error and do it again:

```{r}
diamonds$cut = factor(diamonds$cut, ordered = FALSE)
diamonds$color = factor(diamonds$color, ordered = FALSE)
diamonds$clarity = factor(diamonds$clarity, ordered = FALSE)
modeling_task = TaskRegr$new(id = "diamonds", backend = diamonds, target = "price")
#note the "new" kind of looks like Java or Python. This is part of the new object-oriented package R6
#see https://adv-r.hadley.nz/r6.html if you are interested
res = resample(modeling_task, algorithm, validation) #execute
#nicely parallelized for performance!
res
#not much info here
```
We need to provide an error metric of which there are lots:

```{r}
msrs()
#see https://mlr3.mlr-org.com/reference/mlr_measures_regr.rmse.html
#note how they define it without the p in the denominator... more confusion!
```

Let's see the results:

```{r}
res$score(msr("regr.rmse"))

mean(res$score(msr("regr.rmse"))$regr.rmse)
sd(res$score(msr("regr.rmse"))$regr.rmse)
```

We will return to `mlr` later after we finish our discussion on model selection, stepwise modeling and hyperparameter selection.

## Piping

Take a look at this one-liner:

```{r}
set.seed(1984)
mean(head(round(sample(rnorm(1000), 100), digits = 2)))
```

This is hard to read. Of course we can make it easier by using breaklines e.g.

```{r}
mean(
  head(
    round(
      sample(
        rnorm(1000), 
        100
      ), 
      digits = 2
    )
  )
)
```

But it doesn't make it much easier to read. And it probably makes it harder to write.

Enter an idea taken from unix / linux. Output of one function is input to next function. It is the inverse of the usual "order of operations". Let's see how this works.

We first load the piping library:

```{r}
pacman::p_load(magrittr)
```

The package is named after Rene Magritte, the Belgian surrealist artist because he wrote [(Ceci n'est pas un pipe)](https://en.wikipedia.org/wiki/The_Treachery_of_Images) on a painting of a pipe.

In pipe format this would look like:

```{r}
set.seed(1984)
rnorm(1000) %>% #the pipe operator
  sample(100) %>% 
  round(digits = 2) %>% #the first argument is passed in automatically.
  head %>%
  mean
```

That's it! There's nothing more to it other than a gain in readability.

What if we wanted to do something like `mean(rnorm(1000) + 1)`? This `rnorm(1000) %>% +1 %>% mean` doesn't work because I imagine because the basic arithmetic operators couldn't be parsed like normal while there was a pipe. So they invented special pipe functions for this:

```{r}
rnorm(1000) %>% 
  add(1) %>% 
  mean
```

There are other exceptions to the rule too which you'll figure out if you adopt the pipe.

Unfortunately... the world at large hasn't completely accepted this as a way to write R. So feel free to use for yourself. But be careful when using this style with others. There are places where everyone uses the pipes (we will see this when we get to dplyr). Also note the code you write with pipes will be slower than the normal syntax.

# The Grammar of graphics and ggplot

First load the package and the dataset of interest as a dataframe:

```{r}
pacman::p_load(ggplot2, quantreg)
cars = MASS::Cars93 #dataframe
```

ggplot is based on the "Grammar of Graphics", a concept invented by the Statistician / Computer Scientist Leland Wilkinson who worked on SPSS, Tableau and now he works at H20, software that analyzes big data. The reference of interest is [here](http://papers.rgrossman.com/proc-094.pdf). He drew on ideas from John Tukey (one of the great statistician of the previous generation) while he was at Bell Labs, Andreas Buja (one of my professors at Penn) and Jerome Friedman (the professor that taught my data mining course when I was in college at Stanford). 

It is a language that allows us to describe the components of a graphic. Previously, graphics were done in one shot and it was clunky. ggplot is a library written by Hadley Wickham based on this concept. Wickham is probably the most famous person in statistical computing today. He has commit rights in R and is one of the architects of RStudio. He calls grammar of graphics "graphical poems". Here are the basic components:

* an underlying data frame
* an "aesthetic" that maps visualization axes in the eventual plot(s) to variables in the data frame
* a "layer" which is composed of
  - a geometric object
  - a statistical transformation
  - a position adjustment
* a "scale" for each aesthetic
* a "coordinate" system for each aesthetic
* optional "facets" (more graphics)
* optional "labels" for the title, axes title, points, etc.

Don't worry - everything has "smart defaults" in Wickham's implementation so you don't have to worry about most things. We will explore some of the features below. Here's a good [cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

ggplot is layered where each component is an object. The objects are added to each other since the "+" operator is overloaded to accept these additions. This is nice because each component can be saved and reused. The following initialized the graphics:

```{r}
ggplot(cars)
```

Nothing happened - except the plot window went gray (the smart default). This is already rendering a graphic, but since it hasn't been given any of the required information, it has nothing to display. Next we create an aesthetics indicating a one-way plot (one variable only).

```{r}
ggplot(cars) + 
  aes(Price)
```

Notice how it can understand the variable name as an object name.

Since we've given it an aesthetics object, it now knows which variable is the x axis (default). It already knows the ranges of the variable (a smart default) and a default scale and coordinate system (smart defaults).

Usually this is done in one step by passing the aesthetics object into the ggplot:

```{r}
ggplot(cars, aes(Price))
```

Now we need to pick a layer by specifying a geometry. This is a type of plot. Since the predictor type of price is continuous, let's pick the "histogram" using the `geom_histogram` function:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram()
```

This can be customized:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram(binwidth = 1, col = "darkred", fill = "blue", alpha = 0.4)
```

Want to save it for your latex?

```{r}
ggsave("plot.png")
system("open plot.png")
ggsave("plot.pdf")
system("open plot.pdf")
```

Here are some other options besides the histogram:

```{r}
ggplot(cars, aes(Price)) +
  geom_dotplot()
ggplot(cars, aes(Price)) +
  geom_area(stat = "bin", binwidth = 2)
ggplot(cars, aes(Price)) +
  geom_freqpoly()
ggplot(cars, aes(Price)) +
  geom_density(fill = "green", alpha = 0.1)

summary(cars)
```


Can we compare price based on different conditions? Yes, we can subset the data and use color and alpha:

```{r}
ggplot(cars, aes(Price)) +
  geom_density(data = subset(cars, Man.trans.avail == "Yes"), col = "grey", fill = "darkgreen", alpha = 0.4) +
  geom_density(data = subset(cars, Man.trans.avail == "No"), col = "grey", fill = "red", alpha = 0.4)
```

Sidebar: why are cars that have manual transmissions available cheaper?

We can look at this also using a histogram of the conditional distributions:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram(data = subset(cars, Man.trans.avail == "Yes"), binwidth = 1, col = "grey", fill = "darkgreen", alpha = 0.4) +
  geom_histogram(data = subset(cars, Man.trans.avail == "No"), binwidth = 1, col = "grey", fill = "red", alpha = 0.4)
```

What if the variable is not continuous e.g. Cylinders? We can use a bar graph / bar plot.

```{r}
ggplot(cars, aes(Cylinders)) +
  geom_bar()
```

This is essential frequency by level of the categorical variable.

Now let's move on to looking at one variable versus another variable. For example price by engine power:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price))
```

Since we've given it an aesthetics object, it now knows which variable is the x axis and which variable is the y axis. It already knows the ranges of the variables (a smart default) and a default scale and coordinate system (smart defaults).

Just as before, now we need to pick a layer by specifying a geometry. This is a type of plot. Let's pick the "scatterplot" using the `geom_point` function:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point()
```

Now we have a nice scatterplot. This function uses the inherited data, the inherited aesthetics. Since this "geometry" is a "layer", we can pass in options to the layer.

```{r}
base_and_aesthetics = ggplot(cars, aes(x = Horsepower, y = Price))
base_and_aesthetics + 
  geom_point(col = "red", fill = "green", shape = 23, size = 3, alpha = 0.3)
```

Let's handle names of axes, title and ranges:

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics +
  ggtitle("Average Car Price vs. Engine Power", subtitle = "in the Cars93 dataset") +
  ylab("Price (in $1000's)")
base_and_aesthetics_with_titles +
  geom_point() +
  xlim(0, 400) +
  ylim(0, 50)
  
```

Let's transform the variables:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_x_continuous(trans = "log2")
```

Each unit increase on the x-axis now represent a doubling increase in x (although the whole scale only spans 3 units). But look at how the grid didn't keep up. Let's fix this:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_x_continuous(trans = "log2", breaks = round(seq(0, max(cars$Horsepower), length.out = 6)))
```

We can do the same to the y axis:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans = "log10")
```

Let's look at some more geometries.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth()
```

Here, I've added two geometries on the same aesthetic! This attempts to explain the relationship $f(x)$ using smoothing. Let's go for more.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_rug()
```

This allows us to also see the marginal distributions of the two variables.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_quantile(col = "red") +
  geom_rug()
```

This fits a line and tries to indicate statistical significance of the line. We have *not* covered any statistics in this class yet (ironic!) ... so ignore how the window is generated.

Can we display more than two dimensions? Yes. Let's indicate a third dimension with shape (only works with factors).

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power and Transmission")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail)) +
  geom_smooth() +
  geom_rug()
```

Can we display more than three dimensions? Yes. Let's indicate a fourth dimension with color.

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission & Drivetrain")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain)) +
  geom_smooth() +
  geom_rug()
```

Can we go to a fifth dimension? Maybe?

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission & Drivetrain")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain, size = Weight), alpha = 0.5) + #size?
  geom_smooth() +
  geom_rug()
```

A seventh? We can use text labels adjacent to the scatterplot's points.

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission, Drivetrain,  Weight & #Cylinders")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain, alpha = Weight)) + #size?
  geom_text(aes(label = Cylinders), vjust = 1.5, col = "darkgrey", lineheight = 0.3, size = 3) +
  geom_smooth() +
  geom_rug()
```

Getting difficult to see what's going on.

Let's move away from the scatterplot to just density estimation:

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power") #reset the title
base_and_aesthetics_with_titles +
  geom_density2d()
```

Other alternatives:

```{r}
base_and_aesthetics_with_titles +
  geom_bin2d(binwidth = c(8, 3))
pacman::p_load(hexbin)
base_and_aesthetics_with_titles +
  geom_hex()
```

This is like a two-dimensional histogram where the bar / hexagon heights are seen with color.

What if the x-axis is categorical for example Cylinders versus price? Typical is the "box and whiskers" plot:

```{r}
ggplot(cars, aes(x = Cylinders, y = Price)) +
  geom_boxplot()
```

Clear relationship!

How about multiple subplots based on the subsetting we did in the histograms? This is called "faceting". Here are two bivariate visualizations laid horizontally:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  geom_smooth() +
  facet_grid(. ~ Man.trans.avail)
```

Or alternatively, vertically:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  geom_smooth() +
  facet_grid(Man.trans.avail ~ .)
```

And we can even double-subset:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  facet_grid(Man.trans.avail ~ Origin)
```

And we can even triple-subset or more:

```{r}
cars$MedWeight = ifelse(cars$Weight > median(cars$Weight), ">MedWeight", "<MedWeight")
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  facet_grid(Man.trans.avail ~ Origin + MedWeight, scales = "free")
```

These three varibles seem somewhat independent.

There are other primitives like `geom_abline` which graphs a line and `geom_segment` we will see today. Note that if you want plots rendered within functions or loops you have to explicitly call the `plot` function:

```{r}
for (nsim in 1 : 3){
  graphics_obj = ggplot(data.frame(x = rnorm(1000))) + 
    geom_histogram(aes(x))
  graphics_obj
}
```

versus:

```{r}
for (nsim in 1 : 3){
  graphics_obj = ggplot(data.frame(x = rnorm(1000))) + geom_histogram(aes(x))
  plot(graphics_obj)
}
```


Lastly, ggplot offers lots of nice customization themes:

```{r}
graphics_obj = base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_quantile(col = "red") +
  geom_rug()
graphics_obj + theme_bw()
graphics_obj + theme_dark()
graphics_obj + theme_classic()

```

Packages offer even more:

```{r}
pacman::p_load(forcats, lazyeval, ggthemes)
graphics_obj + theme_economist()
graphics_obj + theme_stata()
graphics_obj + theme_tufte()
```

and of course, the whimsical one and only:


```{r}
pacman::p_load(xkcd, extrafont)
download.file("http://simonsoftware.se/other/xkcd.ttf", dest = "xkcd.ttf", mode = "wb")
#MAC
# system("mv xkcd.ttf /Library/Fonts")
# font_import(path = "/Library/Fonts", pattern = "xkcd", prompt = FALSE)
# fonts()
# fonttable()
# loadfonts()
#WINDOWS
font_import(path = ".", pattern = "xkcd", prompt = FALSE)
fonts()
fonttable()

loadfonts(device="win")

graphics_obj + theme_xkcd()
```
