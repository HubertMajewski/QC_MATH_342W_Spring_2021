---
title: "Practice Lecture 11 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 3, 2020"
---



# Model Selection and Three Data Splits

This unit is split into three use cases (1) Selection among M explicit models (2) Hyperparameter Selection within one algorithm (3) Stepwise Model Construction

## Selecting one of M Models

We have now covered non-linearities (e.g. polynomial terms) and interactions. A new complication now clearly emerges. If I have $p$ predictors, there are many linear least squares models I can build (considering non-linear least squares models makes the space of models even larger!!)

For instance, here are a bunch of models:

```{r}
mod1 = lm(price ~ carat + depth, diamonds) #using a subset of the features
mod2 = lm(price ~ ., diamonds) #using a subset of the features
mod3 = lm(price ~ poly(carat, 2) + poly(depth, 2), diamonds) #using some polynomial terms
mod4 = lm(price ~ . * ., diamonds) #using all interactions
coef(mod1)
coef(mod2)
coef(mod3)
coef(mod4) %>% length
```

Which model is "best"? 

This is one of the most fundamental problems in statistics, and possibly all of science! 

In class, we discussed validation via dividing $\mathbb{D}$ into (a) a training set and a (b) testing set. Now, we will further divide the training set into (a) a sub-training set and a (b) selection set and we still have the (c) test set. 

The total training set together will fit a model and testing will estimate future performance. But within the total training set, we'll use an elaborate algorithim: we'll fit many models and take the best one. That's the "master algorithm".

We'll make the selection set and the test set the same size but we don't have to. First split up the data:

```{r}
n = nrow(diamonds)
K = 5
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

#make sure we did this right:
pacman::p_load(testthat)
expect_equal(1 : n, sort(c(train_indices, select_indices, test_indices)))

diamonds_train = diamonds[train_indices, ]
diamonds_select = diamonds[select_indices, ]
diamonds_test = diamonds[test_indices, ]

rm(test_indices, select_indices, train_indices)
```

Now, fit all models and select the best one:

```{r}
mod1 = lm(price ~ carat + depth, diamonds_train) #using a subset of the features
mod2 = lm(price ~ ., diamonds_train) #using a subset of the features
mod3 = lm(price ~ poly(carat, 2) + poly(depth, 2), diamonds_train) #using some polynomial terms
mod4 = lm(price ~ . * ., diamonds_train) #using all interactions
```

Now predict on the selection set and look at the oos $s_e$, and select the "best" model

```{r}
yhat_select_mod1 = predict(mod1, diamonds_select)
yhat_select_mod2 = predict(mod2, diamonds_select)
yhat_select_mod3 = predict(mod3, diamonds_select)
yhat_select_mod4 = predict(mod4, diamonds_select)
y_select = diamonds_select$price #the true prices

s_e_s = c(
  sd(yhat_select_mod1 - y_select), 
  sd(yhat_select_mod2 - y_select), 
  sd(yhat_select_mod3 - y_select), 
  sd(yhat_select_mod4 - y_select)
)
names(s_e_s) = paste("mod", 1 : 4, sep = "")
s_e_s
#find the minimum
names(which.min(s_e_s))
```

Which are overfit? Which are underfit? Were these models "poor choices"?

Can we go back and fit some more models? 

Yes - as long as we don't open the "lockbox" of the test set. Let's look at one more model. An expansion of the best of the previous 4 models now with a couple interactions we are convinced are real plus a couple of non-linear terms:

```{r}
mod5 = lm(price ~ 
            . + 
            carat * color + 
            carat * depth + 
            I(carat^2) +
            I(depth^2),
          diamonds_train) 

yhat_select_mod5 = predict(mod5, diamonds_select)

s_e_s = c(s_e_s, sd(yhat_select_mod5 - y_select))
names(s_e_s)[5] = "mod5"
s_e_s
#find the minimum
names(which.min(s_e_s))
```

We can go further and fit more and more models but we should always be careful that we don't fit too many as we may optimize to the selection set. Here, we are lucky since the selection set is large (~11,000 observations) so this is not too much of a fear.

But you can see the problem - how can we build a good model??

The answer to this is non-parametric regression. But first, we will cover two other important topics before we get there.

Let us return and complete the exercise by now declaring we are done modeling and we are going to ship model 5. Let us get a conservative estimate of its performance:

```{r}
yhat_test_mod4 = predict(mod4, diamonds_test)
y_test = diamonds_test$price #the true prices
sd(yhat_test_mod4 - y_test)
```

About the same as the selection estimate --- we did not overfit too much to the selection set.

At this point the lockbox is open and we can never return (if we are honest, of course - many people in this business lie so beware).

Now we can build production model 4 with all data to ship:

```{r}
mod_final = lm(price ~ . * ., diamonds)
```

No evaluation is done on the final model. It is only used to predict future diamonds' prices.

Two improvements using CV to the above:

* To reduce variance in the selection process, you make a CV of the selection set. 
* To reduce variance in the testing process, you make an outer CV of the test set. This is a lot more coding!

Can we use MLR for Linear Model Selection?

Yes, but it is not as nice as I would've liked but it sure beats doing it yourself. I've figured it out by creating my own custom code.

First, we create the task:

```{r}
modeling_task = makeRegrTask(data = diamonds, target = "price") #instantiate the task
```

We now pick the linear models we wish to investigate. Each entry is the r.h.s of the formula that is passed to lm.

```{r}
ALL_LINEAR_MODELS = c(
  ".", 
  "carat * .", 
  ". * ."
  )
```

Now we create a new learner which is a wrapper for the linear model with a custom formula. We need to specify learning parameters, a training function (build g) and a predict function. Then we need to add theese functions to the namespace in a way mlr understands.

```{r}
makeRLearner.regr.custom_ols = function() {
  makeRLearnerRegr(
    cl = "regr.custom_ols",
    package = "base",
    par.set = makeParamSet(
      makeDiscreteLearnerParam(id = "formula_rhs", default = ".", values = ALL_LINEAR_MODELS)
    ),
    properties = c("numerics", "factors", "ordered"),
    name = "Custom OLS with a Formula",
    short.name = "custom_ols"
  )
}

trainLearner.regr.custom_ols = function(.learner, .task, .subset, .weights = NULL, ...){
  formula = as.formula(paste(
    getTaskDesc(.task)$target,
    "~",
    list(...)$formula_rhs #this is passed in the ... as an extra argment
  ))
  
  lm(formula, data = getTaskData(.task, .subset))
}

predictLearner.regr.custom_ols = function (.learner, .model, .newdata, ...){
    predict(.model$learner.model, newdata = .newdata, ...)
}

registerS3method("makeRLearner", "regr.custom_ols", makeRLearner.regr.custom_ols)
registerS3method("trainLearner", "regr.custom_ols", trainLearner.regr.custom_ols)
registerS3method("predictLearner", "regr.custom_ols", predictLearner.regr.custom_ols)
```

Now we create the "inner loop". Here, we cross validate over the different models. We do this by specifying a "tune wrapper" since technically each formula is considered a tuning paramter / hyperparameter the linear model on this task.

```{r}
all_models = makeParamSet(
  makeDiscreteParam(id = "formula_rhs", default = ".", values = ALL_LINEAR_MODELS)
)
inner_loop = makeResampleDesc("CV", iters = 3)
lrn = makeTuneWrapper("regr.custom_ols", #instantiate the OLS learner algorithm
        resampling = inner_loop, 
        par.set = all_models, 
        control = makeTuneControlGrid(), 
        measures = list(rmse))
```

We now create the outer loop and execute:

```{r}
outer_loop = makeResampleDesc("CV", iters = 5)
r = resample(lrn, modeling_task, resampling = outer_loop, extract = getTuneResult, measures = list(rmse))
```

Now we look at the results a bunch of different ways:

```{r}
r #overall estimate of oos error of the whole procedure if it were used on all of $\mathbb{D}$
print(getNestedTuneResultsOptPathDf(r)) #results of each inner validation over all outer iterations
r$extract #"winning" model for each outer iteration
```

See https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html? for info on inner and outer loop CV.

## (2) Hyperparameter Selection

We load the breast cancer dataset from earlier in the class.

```{r}
cancer = MASS::biopsy %>%
  select(-ID) %>% #drop the useless ID column
  na.omit #drop all rows that are missing
modeling_task = makeClassifTask(data = cancer, target = "class") #instantiate the task
```

We now create the SVM using package `e1071` which plugs nicely into `mlr`.

```{r}
pacman::p_load(e1071)
algorithm = makeLearner("classif.svm", kernel = "linear")
```

Now we create the inner loop where we try many different values of the hyperparameter.

```{r}
all_lambdas = 2^(seq(-10, 10, by = 0.5))
all_hyperparams = makeParamSet(
  makeDiscreteParam(id = "cost", default = 1, values = all_lambdas)
)
inner = makeResampleDesc("CV", iters = 3)
lrn = makeTuneWrapper("classif.svm", 
                      resampling = inner, 
                      par.set = all_hyperparams, 
                      control = makeTuneControlGrid(),
                      measures = list(mmce))
length(all_lambdas)
```

Now we create the outer loop and execute

```{r}
outer = makeResampleDesc("CV", iters = 5)
r = resample(lrn, modeling_task, 
            resampling = outer, 
            extract = getTuneResult,
            measures = list(mmce))
```

Now we look at the results a bunch of different ways:

```{r}
r #overall estimate of oos error of the whole procedure if it were used on all of $\mathbb{D}$
print(getNestedTuneResultsOptPathDf(r)) #results of each inner validation over all outer iterations
r$extract #"winning" model for each outer iteration
```

