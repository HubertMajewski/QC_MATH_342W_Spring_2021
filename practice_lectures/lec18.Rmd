---
title: "Practice Lecture 18 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 12, 2021"
---

# C++ and R

R goes back to 1995 when it was adapted from S (written in 1976 by John Chambers at Bell Labs) with minor modifications. The core of base R is written in C and Fortran. These two languages are the fastest known languages (how to measure "fastest" is a huge debate). Thus, base R is very fast. For instance the `sort` function is as fast as C/Fortran since it immediately calls compiled C/Fortran routines.

However, R code itself that you write is "interpreted" which means it is not compiled until you run it. And it has to compile on-the-fly, making it very slow. Prior to v3.4 (April, 2017) it was even slower since the code wasn't JIT compiled. All this "real CS" stuff you can learn in another class..

One notable place to observe this slowness relative to other languages is in looping. For example:

```{r}
SIZE = 1e6
v = 1 : SIZE
```

Take for example a simple function that computes square roots on each element

```{r}
sqrt_vector = function(v){
  v_new = array(NA, length(v))
  for (i in 1 : length(v)){
    v_new[i] = sqrt(v[i])
  }
  v_new
}
```

How fast does this run? Let's use a cool package called `microbenchmark` that allows us to do an operation many times and see how long it takes each time to get an average:

```{r}
pacman::p_load(microbenchmark)
microbenchmark(
  sqrt_vector(v), 
  times = 10
)
```

Does the apply function help?

```{r}
microbenchmark(
  apply(v, 1, FUN = sqrt), 
  times = 10
)
```

Strange that this takes so long? So it doesn't help... it hurts A LOT. Unsure why... Be careful with apply! 

How much faster in C++ should this be?

Enter the `Rcpp` package - a way to compile little bits (or lotta bits) of C++ on the fly.

```{r}
pacman::p_load(Rcpp)
```


Let's write this for loop function to sqrt-ize in C++. We then  compile it and then save it into our namespace to be called like a regular function. Note that we use C++ classes that are not part of standard C++ e.g. "NumericVector". Rcpp comes build in with classes that are interoperable with R. It's not hard to learn, just takes a small dive into the documentation.

```{r}
cppFunction('
  NumericVector sqrt_vector_cpp(NumericVector v) {
    int n = v.size();
    NumericVector v_new(n);
    for (int i = 0; i < n; i++) { //indices from 0...n-1 not 1...n!
      v_new[i] = sqrt(v[i]);
    }
    return v_new;
  }
')
```

What do these two functions look like?

```{r}
sqrt_vector
sqrt_vector_cpp
```

The first one shows the R code and then says it is bytecode-compiled which means there are speedups used in R (go to an advanced CS class) but we will see these speedups aren't so speedy! The other just says we `.Call` some C++ function in a certain address (pointer) and the argument to be inputted.

What is the gain in runtime?

```{r}
microbenchmark(
  sqrt_vector_cpp(v), 
  times = 10
)
```

WOW. 10x!!! Can't beat that with a stick...

Let's do a not-so-contrived example...

Matrix distance... Let's compute the distances of all pairs of rows in a dataset. I will try to code the R as efficiently as possible by using vector subtraction so there is only two for loops. The C++ function will have an additional loop to iterate over the features in the observations.

```{r}
#a subset of the diamonds data
SIZE = 1000
X_diamonds = as.matrix(ggplot2::diamonds[1 : SIZE, c("carat", "depth", "table", "x", "y", "z")])

compute_distance_matrix = function(X){
  n = nrow(X)
  D = matrix(NA, n, n)
  for (i_1 in 1 : (n - 1)){
    for (i_2 in (i_1 + 1) : n){
      D[i_1, i_2] = sqrt(sum((X[i_1, ] - X[i_2, ])^2))
    }
  }
  D
}

cppFunction('
  NumericMatrix compute_distance_matrix_cpp(NumericMatrix X) {
    int n = X.nrow();
    int p = X.ncol();
    NumericMatrix D(n, n);
    std::fill(D.begin(), D.end(), NA_REAL);

    for (int i_1 = 0; i_1 < (n - 1); i_1++){
      //Rcout << "computing for row #: " << (i_1 + 1) << "\\n";
      for (int i_2 = i_1 + 1; i_2 < n; i_2++){
        double sqd_diff = 0;
        for (int j = 0; j < p; j++){
          sqd_diff += pow(X(i_1, j) - X(i_2, j), 2); //by default the cmath library in std is loaded
        }
        D(i_1, i_2) = sqrt(sqd_diff); //by default the cmath library in std is loaded
      }
    }
    return D;
  }
')
```

```{r}
microbenchmark(
  {D = compute_distance_matrix(X_diamonds)},
  times = 10
)

round(D[1 : 5, 1 : 5], 2)
```

Slow...

```{r}
microbenchmark(
  {D = compute_distance_matrix_cpp(X_diamonds)},
  times = 10
)
round(D[1 : 5, 1 : 5], 2)
```

Absolutely lightning... ~200x faster on my laptop than R's runtime.

Writing functions as strings that compile is annoying. It is better to have separate files. For instance...

```{r}
sourceCpp("distance_matrix.cpp")
```

Here are a list of the data structures in Rcpp: https://teuder.github.io/rcpp4everyone_en/070_data_types.html#vector-and-matrix

Another place where C++ pays the rent is recursion. Here is a quicksort implementation in R taken from somewhere on the internet.

```{r}
quicksort_R <- function(arr) {
  # Pick a number at random.
  mid <- sample(arr, 1)

  # Place-holders for left and right values.
  left <- c()
  right <- c()
  
  # Move all the smaller values to the left, bigger values to the right.
  lapply(arr[arr != mid], function(d) {
    if (d < mid) {
      left <<- c(left, d)
    }
    else {
      right <<- c(right, d)
    }
  })
  
  if (length(left) > 1) {
    left <- quicksort_R(left)
  }
  
  if (length(right) > 1) {
    right <- quicksort_R(right)
  }
  
  # Finally, return the sorted values.
  c(left, mid, right)
}
```

Let's create a random array to test these sorts on:

```{r}
n = 10000
x = rnorm(n)
```


Let's profile the pure R sort function:

```{r}
microbenchmark(
  x_sorted_pure_R = quicksort_R(x),
  times = 10
)
```

Let's profile R's `sort` function.

```{r}
microbenchmark(
  x_sorted_base_R = sort(x),
  times = 10
)
```

Let's just ensure our method worked...

```{r}
x_sorted_pure_R = quicksort_R(x)
x_sorted_base_R = sort(x)
pacman::p_load(testthat)
expect_equal(x_sorted_pure_R, x_sorted_base_R)
```

Basically infinitely faster. Let's make our own C++ implementation.

```{r}
sourceCpp("quicksort.cpp")
```

and profile it:

```{r}
microbenchmark(
  x_sorted_cpp = quicksort_cpp(x),
  times = 10
)
```

Let's just ensure this method worked...

```{r}
pacman::p_load(testthat)
expect_equal(x_sorted_cpp, x_sorted_base_R)
```

Why is our C++ slower than `sort`. Because `sort` is also in C++ or Fortran and it's been likely optimized and reoptimized up to wazoo for decades. Also, Rcpp's data structures may be slower than base R's data structures. There may be some speed lost to translating to `NumericVector` from `double[]` or something like that.

Can you call R from Rcpp? You bet:

```{r}
cppFunction('
  NumericVector rnorm_cpp_R(int n, double mean, double sd){
      // get a pointer to R\'s rnorm() function
      Function f("rnorm");   
  
      // Next code is interpreted as rnorm(n, mean, sd)
      return f(n, Named("sd")=sd, _["mean"]=mean);
  }
')

rnorm_cpp_R(5, 1, .01)
```

A few math functions are implemented for you already:

```{r}
evalCpp('R::qnorm(0.5, 0, 1, 1, 0)')
evalCpp('R::qnorm(0.5, 0, 1)') #BOOM
```

Further, there are many common functions that are already wrapped for you via "Rcpp-sugar" which was the Rcpp's author's attempt to make Rcpp a whole lot easier, see [here](http://dirk.eddelbuettel.com/code/rcpp/Rcpp-sugar.pdf).

```{r}
evalCpp('rnorm(10, 100, 3)')
```

If you want blazing fast linear algebra, check out package `RcppArmadillo` which is a wrapper around Apache's Armadillo (namespace is "arma" in the code), an optimized linear algebra package in C++. Here is an example taken from [here](https://scholar.princeton.edu/sites/default/files/q-aps/files/slides_day4_am.pdf). It involves solving for b-vec in a standard OLS.

```{r}
pacman::p_load(RcppArmadillo)

cppFunction('
  arma::mat ols_cpp(arma::mat X, arma::mat y){
    arma::mat Xt = X.t();
    return solve(Xt * X, Xt * y);
  }
', depends = "RcppArmadillo")

n = 500
Xy = data.frame(int = rep(1, n), x1 = rnorm(n), x2 = rnorm(n), x3 = rnorm(n), y = rnorm(n))
X = as.matrix(Xy[, 1 : 4])
y = as.matrix(Xy[, 5])

#does the function work?
expect_equal(as.numeric(ols_cpp(X, y)), as.numeric(solve(t(X) %*% X) %*% t(X) %*% y))
```

Now how fast is it?

```{r}
microbenchmark(
  R_via_lm = lm(y ~ 0 + ., data = Xy),
  R_matrix_multiplication = solve(t(X) %*% X) %*% t(X) %*% y,
  cpp_with_armadillo = ols_cpp(X, y),
    times = 100
)
```

About 4x faster than R's optimized linear algebra routines. Supposedly it can go even faster if you enable parallelization within Armadillo. I couldn't get that demo to work...

Note lm is slow because it does all sorts of other stuff besides computing b-vec e.g. builds the model matrix, computes Rsq, computes residuals, does statistical testing, etc...

Here are the places where Rcpp is recommended to be used (from https://teuder.github.io/rcpp4everyone_en/010_Rcpp_merit.html)

* Loop operations in which later iterations depend on previous iterations.
* Accessing each element of a vector/matrix.
* Recurrent function calls within loops.
* Changing the size of vectors dynamically.
* Operations that need advanced data structures and algorithms (we don't do this in this class).

# Java and R

We just did C++ with R. Is there a bridge to Java? Yes (and there's bridges to many other languages too). Java and R can speak to each other through proper configuration of the `rJava` package. You need to have a full JDK of Java installed on your computer and have its binary executables in the proper path. This demo will be in Java JDK 8 (released in 2014 and not officially supported after 2020) since I haven't tested on the more modern Java JDK's yet. We first install `rJava` if necessary:

```{r}
if (!pacman::p_isinstalled(rJava)){
  pacman::p_load(pkgbuild)
  if (pkgbuild::check_build_tools()){
    install.packages("rJava", type = "source")
  }
  install.packages("rJava")
}
```

Now we load the package. Before we do, we set the JVM to have 4G of RAM. After we load it, we initialize te JVM. This should print out nothing or "0" to indicate success.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
```

Just like the whole `Rcpp` demo, we can do a whole demo with `rJava`, but we won't. Here's just an example of creating a Java object and running a method on it:

```{r}
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue") #java_double.intValue();
#call a static method
J("java/lang/String", "valueOf", java_double)
```

A note on rJava vs Rcpp. 

* If you're doing quick and dirty fast functions for loops and recursion, do it in Rcpp since there is lower overhead of programming. 
* If you are programming more full-featured software, go with rJava. 
* Also, if you need full-featured parallelized execution and threading control e.g. thread pooling and the ease of debugging, my personal opinion is that rJava is easier to get working with less dependencies. My experience is that the Rcpp threading libraries just aren't there yet and neither is openMP directives within Rcpp. 
* Further, the JVM is fully asynchronous which means it runs completely independently of R. What this means is that you can execute something in Java, Java can "thread it off" and return you to the R prompt with a pointer to the object that houses its execution. You can then query the object. We will see dems of this.

## Data "Munging" with Dplyr and data.table

"Data munging", sometimes referred to as "data wrangling", is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. A data wrangler is a person who performs these transformation operations. -[Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling).

Half of what a data scientist does is cleaning data, visualizing data and wrangling it. In the process you learn all about your dataset and you're on a higher level when it comes time to build prediction models.

The packages `dplyr` and `data.table` offer many conveninent functions to manipulate, clean, and otherwise wrangle data. Note: all the wrangling we're going to see *can* be done with base R (see previous notes on the `data.frame` object) but it would be *very very very very* annoying and *very very very very* slow.

I will quickly compare and contrast `dplyr` and `data.table` before you see it inside actual code.

* `dplyr` works really nicely with the piping chain as you "begin" the manipulation with the dataset and then iteratively pipe in step 1, step 2, etc until you wind up with what end product you would like. This makes `dplyr` very readable but very verbose - lots of lines of code. 
* On the flip side, `data.table` essentially wrote a new data wrangling language so it's a harder learning curve but it's very compact - very few lines of code.
* `data.table` is blazing fast and kills `dplyr` in performance and I'm pretty sure it even beats Python in performance (someone please check this). So in the era of "big data", I think this is the winner even though it is much harder to learn.
* I believe `dplyr` is more popular in the real world and thus has more cache to put on your CV. But this is constantly in flux!

For all labs and the final project, you are recommended to pick one you want to use and go with it. For the exams, I will write code in both (if need be) to not penalize / reward a student who picked one over the other.

Here is a nice [translation guide](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) between `dplyr` and `data.table`. We will be learning them in tandem. I could've split this into two units but I decided against it because (1) it is good to see the same functionality side-by-side and (2) this is really just one concept.

```{r}
pacman::p_load(tidyverse, magrittr) #tidyverse is shorthard for dplyr, ggplot2, tidyr, readr and a bunch of other packages recommended for the "full" dplyr experience. I'm using magrittr for special pipe operations later.
pacman::p_load(data.table)
```

First, recall what pipe format means! We're going to need to know this well for what's coming next...

```{r}
set.seed(1984)
mean(head(round(sample(rnorm(1000), 100), digits = 2)))

set.seed(1984)
rnorm(1000) %>% #the pipe operator
  sample(100) %>% 
  round(digits = 2) %>% #the first argument is passed in automatically.
  head %>%
  mean
```

Note that `data.table` is automatically multithreaded. Read [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/setDTthreads).

```{r}
getDTthreads()
```


We first instantiate the upgraded data.frame objects in both libraries:

```{r}
diamonds_tbl = tbl_df(diamonds) #not necessary to cast because dplyr does the conversion automatically after using any dplyr function
diamonds_dt = data.table(diamonds) #absolutely necessary
```

What happens during the data frame conversion?

```{r}
class(diamonds_tbl)
class(diamonds_dt)
```

Note how these are implemented as class extensions of R's `data.frame` as to allow for background compatibility and not break the API. They have nicer ways of showing the data:

```{r}
diamonds_tbl #run this in the console, not inside the chunk
diamonds_dt #run this in the console, not inside the chunk
```

Beginning with the simplest munging tasks, subsetting rows:

```{r}
diamonds_tbl %>% 
  slice(1 : 5)

diamonds_dt[1 : 5]
```

And subsetting columns:

```{r}
diamonds_tbl %>% 
  select(cut, carat, price) #these three only in this order

diamonds_dt[, .(cut, carat, price)]

diamonds_tbl %>% 
  select(carat, price, cut) #these three only in another order

diamonds_dt[, .(carat, price, cut)]

diamonds_tbl %>% 
  select(-x) #drop this feature

diamonds_dt[, !"x"]
#diamonds_dt[, x := NULL] #mutating function (overwrites the data frame)

diamonds_tbl %>% 
  select(-c(x, y, z)) #drop these features
diamonds_tbl %>% 
  select(-x, -y, -z) #drop these features

diamonds_dt[, !c("x", "y", "z")]
```

How about will rename a column

```{r}
diamonds_tbl %>% 
  rename(weight = carat, price_USD = price)

diamonds_dt_copy = copy(diamonds_dt)
setnames(diamonds_dt_copy, old = c("carat", "price"), new = c("weight", "price_USD")) #the `setnames` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

If you want to rearrange the columns...

```{r}
#In dplyr you pretend to select a subset and then ask for everything else:
diamonds_tbl %>% 
  select(carat, price, cut, everything()) #these three in this order first then everything else
# diamonds_tbl %>% 
#   select(-carat, everything()) #move carat last (first drop it, and then add it back in with everything)

diamonds_dt_copy = copy(diamonds_dt)
setcolorder(diamonds_dt_copy, c("carat", "price", "cut")) #as before, the `setcolorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

Sorting the rows by column(s):

```{r}
diamonds_tbl %>%
  arrange(carat) #default is ascending i.e. lowest first

diamonds_dt[order(carat)]
diamonds_dt_copy = copy(diamonds_dt)
setorder(diamonds_dt_copy, carat) #as before, the `setorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)

diamonds_tbl %>%
  arrange(desc(carat)) #switch to descending, i.e. highest first

diamonds_dt[order(-carat)] #and you can do this with `setorder` too

diamonds_tbl %>%
  arrange(desc(color), clarity, cut, desc(carat)) #multiple sorts - very powerful

diamonds_dt[order(-color, clarity, cut, -carat)] #and you can do this with `setorder` too
```

The filter method subsets the data based on conditions:

```{r}
diamonds_tbl %>%
  filter(cut == "Ideal")

diamonds_dt[cut == "Ideal"]

diamonds_tbl %>%
  filter(cut == "Ideal") %>%
  filter(depth < 65) %>%
  filter(x * y * z > 20)
diamonds_tbl %>%
  filter(cut == "Ideal" & depth < 65 & x * y * z > 20)

diamonds_dt[cut == "Ideal" & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter((cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[(cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter(cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20]
```

How about removing all rows that are the same?

```{r}
diamonds_tbl
diamonds_tbl %>%
  distinct

unique(diamonds_dt)

#nice function from data.table:
uniqueN(diamonds$carat) 
#273 < 53940 i.e. there's only a few weight measurements that are possible... let's only keep one from each unique carat value

diamonds_tbl %>%
  distinct(carat, .keep_all = TRUE) #keeps the first row for each unique weight measurement

unique(diamonds_dt, by = "carat")
```

Sampling is easy

```{r}
diamonds_tbl %>%
  sample_n(7)

diamonds_dt[sample(.N, 7)] #.N is a cool function: it is short for `nrow(dt object)`

diamonds_tbl %>%
  sample_frac(1e-3)

diamonds_dt[sample(.N, .N * 1e-3)] #.N is a cool function: it is short for `nrow(dt object)
```


Now for some real fun stuff. Let's create new features with the `mutate` function.

```{r}
diamonds_tbl %>%
  mutate(volume = x * y * z) #adds a new column keeping the old ones (this was an exam problem in a previous year)

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, volume := x * y * z]
diamonds_dt2

diamonds_tbl %>%
  mutate(price_per_carat = price / carat) %>%
  arrange(desc(price_per_carat))

diamonds_dt2[, price_per_carat := price / carat]
diamonds_dt2[order(-price_per_carat)]
rm(diamonds_dt2)
```

Or rewrite old ones.

```{r}
diamonds_tbl %>%
  mutate(cut = substr(cut, 1, 1))

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, cut := substr(cut, 1, 1)]
diamonds_dt2

diamonds_tbl %>%
  mutate(carat = factor(carat))

diamonds_dt2[, carat := factor(carat)]
diamonds_dt2
rm(diamonds_dt2)
```

Here are some more ways to create new variables. Translating to `data.table` is trivial:

```{r}
diamonds_tbl %>%
  mutate(carat = factor(ntile(carat, 5)))
diamonds_tbl %>%
  mutate(carat = percent_rank(carat))
diamonds_tbl %>%
  mutate(lag_price = lag(price)) #if this data was a time series
diamonds_tbl %>%
  mutate(cumul_price = cumsum(price)) #%>% tail
```

How about if you want to create a column and drop all other columns in the process?

```{r}
diamonds_tbl %>%
  transmute(volume = x * y * z) #adds a new column dropping the old ones

diamonds_dt[, .(volume = x * y * z)]
```

There are many ways to reshape a dataset. We will see two now and a few functions later when it becomes important. For instance: we can collapse columns together using the `unite` function from package `tidyr` (which should be loaded when you load `dplyr`). We will have a short unit on more exciting and useful reshapings later ("long" to "short" and vice-versa). As far as I know `data.table` has a less elegant... unless someone has a better idea?

```{r}
diamonds_tbl2 = diamonds_tbl %>%
  unite(dimensions, x, y, z, sep = " x ")
diamonds_tbl2

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, dimensions := paste(x, y, z, sep = " x ")] #mutating
diamonds_dt2 = diamonds_dt2[, !c("x", "y", "z")]
diamonds_dt2
```

We can reverse this operation:

```{r}
diamonds_tbl2 %>%
  separate(dimensions, c("x", "y", "z"), sep = " x ")
rm(diamonds_tbl2)

diamonds_dt2[, c("x", "y", "z") := strsplit(dimensions, "x")]
diamonds_dt2[, -"dimensions"]
rm(diamonds_dt2)
```

There are tons of other packages to do clever things. For instance, here's one that does dummies. Let's convert the color feature to dummies. Again slightly less readable or elegant in `data.table`:

```{r}
pacman::p_load(sjmisc, snakecase)
diamonds_tbl %>%
  to_dummy(color, suffix = "label") %>% #this creates all the dummies
  bind_cols(diamonds_tbl) %>% #now we have to add all the original data back in
  select(-matches("_"), everything()) %>% #this puts the dummies last
  select(-color) #finally we can drop color

cbind(
  diamonds_dt[, -"color"], 
  to_dummy(diamonds_dt[, .(color)], suffix = "label")
)
```


What if you want to create a new variable based on functions only run on subsets of the data. This is called "grouping". Grouping only makes sense for categorical variables. (If you group on a continuous variable, then chances are you'll have $n$ different groups because you'll have $n$ unique values).

For instance:

```{r}
diamonds_tbl %>%
  group_by(color)

diamonds_dt[,, by = color]
```

Nothing happened... these were directives to do things a bit differently with the addition of other logic. So after you group, you can now run operations on each group like they're their own sub-data frame. Usually, you want to *summarize* data by group. This means you take the entire sub-data frame and run one metric on it and return only those metrics (i.e. shrink $n$ rows to $L$ rows). This sounds more complicated than it is and it is where data wrangling really gets fun. 

Here are a few examples:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price))

diamonds_dt[, .(avg_price = mean(price)), by = color][order(color)] #chaining / piping [...][...][...] etc
#where did all the other rows and columns go???

diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price), sd_price = sd(price), count = n())

diamonds_dt[, .(avg_price = mean(price), sd_price = sd(price), count = .N), by = color][order(color)]

diamonds_tbl %>%
  group_by(color) %>%
  summarize(min_price = min(price), med_price = median(price), max_price = max(price))

diamonds_dt[, .(min_price = min(price), med_price = median(price), max_price = max(price)), by = color][order(color)]
```

Sometimes you want to do fancier things like actually run operations on the whole sub-data frame using `mutate`. If the function is a single metric, then that metric is then duplicated across the whole sub data frame.

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(avg_price_for_color = mean(price))
#creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, avg_price_for_color := mean(price), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

So that's kind of like duplicating a summary stat. Here's something more fun: actually creating a new vector:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(price_rank_within_color = dense_rank(price)) #creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, price_rank_within_color := frankv(price, ties.method = "dense"), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

What if we want to get the first row in each category?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1)

diamonds_dt[, .SD[1], by = color][order(color)]
```

The `.SD` variable is short for "sub dataframe" and it's a stand-in for the pieces of the dataframe for each color as it loops over the colors. So `.SD[1]` will be first row in the sub dataframe. The reason why the matrices come out different is that the order of the rows in data.table changes based on optimizations. We'll see some of this later. I'm also unsure why it moved the `color` column to the front.

What about first and last?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1, n())

diamonds_dt[, .SD[c(1, .N)], by = color]
```

How about the diamond with the highest price by color?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  arrange(price) %>%
  slice(n())

diamonds_dt[, .SD[which.max(price)], by = color]
```

We've seen `data.table`'s preference for mutating functions. Here is a pipe command from package `magrittr` that makes the functions mutating. 

```{r}
diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 = diamonds_tbl2 %>%
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2

diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 %<>% #pipe and overwrite (short for what's above)
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2
rm(diamonds_tbl2)
```

This is as far we will go with data wrangling right now.

Let's benchmark a few core features of both packages. To do so, let's create a dataframe that's very big:

```{r}
pacman::p_load(microbenchmark)

Nbig = 2e6
diamonds_tbl_big = diamonds_tbl %>%
  sample_n(Nbig, replace = TRUE)
diamonds_dt_big = data.table(diamonds_tbl_big) #just to make sure we have the same data
diamonds_big = data.frame(diamonds_tbl_big) #ensure that it is a base R object
```

How about we write this dataframe to the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = write.csv(diamonds_big, "diamonds_big.csv"),
  tidyverse = write_csv(diamonds_tbl_big, "diamonds_big.csv"),
  data.table = fwrite(diamonds_dt_big, "diamonds_big.csv"),
    times = 1
)
```

How about we read this dataframe from the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = read.csv("diamonds_big.csv"),
  tidyverse = read_csv("diamonds_big.csv"),
  data.table = fread("diamonds_big.csv"),
    times = 1
)
```

What about for creating new variables?

```{r}
microbenchmark(
  base_R = {diamonds_big$log_price = log(diamonds_big$price)},
  tidyverse = {diamonds_tbl_big %<>% mutate(log_price = log(price))},
  data.table = diamonds_dt_big[, log_price := log(price)],
    times = 100
)
```

About the same. How about grouping and summarizing? No easy one-liner in base R. So we just compare the two packages:


```{r}
microbenchmark(
  tidyverse = {diamonds_tbl_big %>% group_by(color) %>% summarize(avg_price = mean(price))},
  data.table = diamonds_dt_big[, .(avg_price = mean(price), by = color)],
    times = 10
)
```

How about sorting?

```{r}
microbenchmark(
  base_R = diamonds_big[order(diamonds_big$price), ],
  tidyverse = {diamonds_tbl_big %>% arrange(price)},
  data.table = diamonds_dt_big[order(price)],
    times = 10
)
```
How about filtering?

```{r}
microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 10
)
```

Let's do this again but first "key" the price column which is what you would do if you are doing lots of searches.

```{r}
setkey(diamonds_dt_big, price)

microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 30
)
```

We still have to learn how to reshape tables and join multiple tables together. We will do that later.

