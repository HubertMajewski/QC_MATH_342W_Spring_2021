---
title: "Practice Lecture 16 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 5, 2021"
---

## The K tradeoff

K determines how large the training set is relative to the test set when you're doing honest validation for an algorithm. For now, let's not use K-fold CV, but only examine one split at a time. Consider this simulated dataset with 50 observations:

```{r}
n = 50
xmin = 0
xmax = 4
# set.seed(1)
set.seed(1984)
x = runif(n, xmin, xmax)
y = 2 + 3 * x^2 + rnorm(n, 0, 0.8)
Xy = data.frame(x = x, y = y)
pacman::p_load(ggplot2)
data_plot = ggplot(Xy) + aes(x = x, y = y) + geom_point()
data_plot
```

Note how $f(x)$ is quadratic and there is random noise which is "ignorance error". The random noise will be part of generalization error and can never go away.

If we use OLS with no derived features, then we can at most get $h*(x)$. Let's see what $h^*(x) = \beta_0 + \beta_1 x$ truly is. To do this, we imagine we see an absolute ton of data and run OLS on it.

```{r}
n_hidden = 1e6
x_hidden = seq(from = xmin, to = xmax, length.out = n_hidden)
y_hidden = 2 + 3 * x_hidden^2 + rnorm(n_hidden, 0, 0.8)
h_star_mod = lm(y_hidden ~ x_hidden)
coef(h_star_mod)
```

The fact that $\beta = [-6~12]^\top$ can actually be solved with calculus: $\int_0^4 ((2 + 3x^2) - (b_0 + b_1 x))^2 dx$ and solve for $b0$ and $b1$ explicitly by minimizing.

Plotting that over $\mathbb{D}$ we obtain

```{r}
data_plot +
  geom_abline(intercept = coef(h_star_mod)[1], slope = coef(h_star_mod)[2], color = "green")
```

That is the best we're going to get. However, $g_{final}$ falls far short of it:

```{r}
g_final_mod = lm(y ~ x)
coef(g_final_mod)
```


The actual error of g_final can be estimated by imagining tons of future observations:

```{r}
y_hat_g_final = predict(g_final_mod, data.frame(x = x_hidden))
gen_error_true = sd(y_hidden - y_hat_g_final)
gen_error_true
```

The model $g$ can vary quite a bit as we subsample $\mathbb{D}$ which is what happens when you do train-test splits. It varies a lot because there is large misspecification error. If the model was correctly specified, the results of everything that follows will be less impressive. But in the real world - is your model ever correctly specified? is $f \in \mathcal{H}$?? NO. So this is more realistic.

Now let's let K be small. Let K = 2 meaning even 50-50 split of test and train.

```{r}
K = 2
prop_train = (K - 1) / K
n_train = round(prop_train * n)
set.seed(123)
index_train = sample(1 : n, n_train, replace = FALSE)
index_test = setdiff(1 : n, index_train)
pacman::p_load(testthat)
expect_equal(sort(c(index_test, index_train)), 1:n)

x_train = x[index_train]
y_train = y[index_train]
Xytrain = data.frame(x = x_train, y = y_train)
x_test = x[index_test]
y_test = y[index_test]

g_mod = lm(y ~ ., Xytrain)
y_hat_g = predict(g_mod, data.frame(x = x_test))
g_s_e_K_2 = sd(y_test - y_hat_g)
g_s_e_K_2
gen_error_true
```

Although I cooked the books by setting the seed, this realization makes sense. If K=2, I build the model g with half the data than the model g_final. Less data to train on => higher generalization error. How about if K is large. Let's say $K = n / 2$ meaning n_train = 48 and n_test = 2.

```{r}
K = n / 2
prop_train = (K - 1) / K
n_train = round(prop_train * n)
set.seed(123)
index_train = sample(1 : n, n_train, replace = FALSE)
index_test = setdiff(1 : n, index_train)
pacman::p_load(testthat)
expect_equal(sort(c(index_test, index_train)), 1:n)

x_train = x[index_train]
y_train = y[index_train]
Dtrain = data.frame(x = x_train, y = y_train)
x_test = x[index_test]
y_test = y[index_test]

g_mod = lm(y ~ ., Dtrain)
y_hat_g = predict(g_mod, data.frame(x = x_test))
g_s_e_K_n_over_2 = sd(y_test - y_hat_g)
g_s_e_K_2
g_s_e_K_n_over_2
gen_error_true
```

Although I cooked the books again by setting the seed, this also makes sense. More data to train on = less error but still more error than all the data. In reality, there is massive variance over specific splits! Let's run the simulation with these two K values many times.

While we're at it, let's do all K's! Well, what are all the valid K's? If you want to keep the sizes the same, any factorization of n except the trivial 1 since n = 1 * n. A K = 1 would mean there's no split!!! How to find divisors? Of course a package for this.

```{r}
pacman::p_load(numbers)
setdiff(divisors(n), 1)
```

But should we also include the trivial n? Yes K = n is indeed a valid divisor. And this type of CV is called the "leave one out cross validation" (LOOCV). Now we compute the errors over K:


```{r}
Nsim_per_K = 2000
Kuniqs = setdiff(divisors(n), 1)
num_Kuniqs = length(Kuniqs)
Ks = rep(Kuniqs, Nsim_per_K)
results = data.frame(s_e = rep(NA, Nsim_per_K * num_Kuniqs), K = rep(NA, Nsim_per_K * num_Kuniqs))
for (i in 1 : length(Ks)){
  K = Ks[i]
  prop_train = (K - 1) / K
  n_train = round(prop_train * n)
  index_train = sample(1 : n, n_train, replace = FALSE)
  index_test = setdiff(1 : n, index_train)
  expect_equal(sort(c(index_test, index_train)), 1:n)
  
  x_train = x[index_train]
  y_train = y[index_train]
  Xytrain = data.frame(x = x_train, y = y_train)
  x_test = x[index_test]
  y_test = y[index_test]
  
  g_mod = lm(y ~ ., Xytrain)
  y_hat_g = predict(g_mod, data.frame(x = x_test))
  if (length(y_test) == 1){ #for leave one out cross validation
    g_s_e = abs(y_test - y_hat_g)
  } else {
    g_s_e = sd(y_test - y_hat_g)
  }
  results[i, ] = c(g_s_e, K)
}
```

What are the variabilities? Let's take the average error over each simulated split. 

```{r}
#don't worry about the following code... we will learn dplyr later...
pacman::p_load(dplyr)
results_summary = results %>%
  group_by(K) %>%
  summarize(Kavg = mean(s_e), Kse = sd(s_e))
results_summary
```


Now let's see what the distributions look like to visualize the means and variances.

```{r}
sim_plot = ggplot(results) + 
  aes(x = s_e) +
  geom_density(aes(fill = factor(K)), alpha = 0.3) + 
  xlim(0, NA) + 
  geom_vline(data = results_summary, aes(xintercept = Kavg, color = factor(K)), size = 2)
sim_plot
```

The main takeaways are

(1) the std err of generalization error estimate is much lower for low K than high K

With high K, the test set is small meaning the estimate has high variance; with low K, the test set is large meaning you can measure it with low variance.

(2) the average of generalization error estimate is lower for high K than low K

With high K, the training set is large meaning $g$ is closer to g_final and thus has higher expected accuracy; with low K, the training set is small meaning $g$ is further from g_final and thus has lower expected accuracy.

Thus, the tradeoff is bias vs. variance. There are many similar tradeoffs in statistics. We will see one later when we do machine learning.

Is the estimates' accuracy for what we really care about? No... the generalization error of g_final which we picture below:

```{r}
sim_plot + 
  geom_vline(xintercept = gen_error_true, col = "white", size = 1)
```

Remember, g_final's error should be lower than both averages since it uses all the data. But we see above it's higher!

So what happened? Simple... we are mixing apples and oranges. We calculated that white line by looking at one million future observations. We calculated the red and blue distributions by looking at our data only which is a random realization of many such datasets! Thus, our generalization errors are biased based on the specific n observations in D we received. We will see that K-fold helps a bit with this. But there is nothing we can do about it beyond that (besides collect more observations). If you get a weird sample, you get a weird sample!

How would we be able to generate the picture we really want to see? We would run this simulation over many datasets and average. That would be a giant simulation. To show that this is the case, go back and change the seed in the first chunk and rerun. You'll see a different white bar.

What is the main takeaway? K matters because it induces a tradeoff. It shouldn't be too large or too small (as we believe at the moment). And, generalization error estimation is very variable in low n. To see this, go back and increase n.

## Reducing variance with Cross Validation (i.e. K-fold CV)

We saw previous there was a lot of variance in generalization error estimation. We can reduce some of this variance by using a very simple trick. We can rotate the train-test split so that each observation will be in the test set once. How many times is this done? K. Now we see the reason for the definition of K as it tells you how many times you validate. Why is it called "cross"? Because the training set crosses over as it does the rotation. Each observation is inside a training set K-1 times. This point will become important later. Why is it called K-fold? Because a fold is one set of training-test and there are K unique folds during the whole procedure.

How does this work? Well, let's say K=10, a typical value. This means in each "fold", 90% of the data is in the training set and 10% of the data is in the test set. As we run through the K folds, we train a model on the training set and predict on the test set and compute oos residuals We aggregate those oos residuals over the folds to result in n oos residuals. We then run our error metric on all n.

Let's begin with the dataset from the previous demo. Here is that code that will create the folds by specifying the K=10 test sets by index. The training sets can then be found by the set difference function.

```{r}
K = 10
set.seed(1984)
temp = rnorm(n)
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
table(observation_folds)
```

We now do our first cross validation of the linear model.

```{r}
oos_cv_residuals = array(NA, n)
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 : n, index_test)
  
  x_train = x[index_train]
  y_train = y[index_train]
  Xtrain = data.frame(x = x_train, y = y_train)
  x_test = x[index_test]
  y_test = y[index_test]
  
  g_mod = lm(y ~ ., Xtrain)
  y_hat_g = predict(g_mod, data.frame(x = x_test))
  oos_cv_residuals[index_test] = y_test - y_hat_g
}

sd(oos_cv_residuals)
```

How does this CV error look over K? 

```{r}
Kuniqs = setdiff(divisors(n), 1)
results = data.frame(K = Kuniqs, s_e = NA)

set.seed(1984)
for (K in Kuniqs){
    temp = rnorm(n)
    observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
  oos_residuals = array(NA, n)
  for (k in 1 : K){
    index_test = which(observation_folds == k)
    index_train = setdiff(1 : n, index_test)
    
    x_train = x[index_train]
    y_train = y[index_train]
    Xtrain = data.frame(x = x_train, y = y_train)
    x_test = x[index_test]
    y_test = y[index_test]
    
    g_mod = lm(y ~ ., Xtrain)
    y_hat_g = predict(g_mod, data.frame(x = x_test))
    oos_residuals[index_test] = y_test - y_hat_g
  }
  
  results[results$K == K, "s_e"] = sd(oos_residuals)
}
results
```

Seemingly less variable than previously since before we got 

[1] 4.58069
[1] 3.85186 

where the top value was for K = 2 and the bottom value was for K = 25.

There is still an effect of the one random fold. Let's do this many times and look at the distribution just like before.

```{r}
Nsim_per_K = 500
num_Kuniqs = length(Kuniqs)
Ks = rep(Kuniqs, Nsim_per_K)
results = data.frame(s_e = rep(NA, Nsim_per_K * num_Kuniqs), K = rep(NA, Nsim_per_K * num_Kuniqs))


set.seed(1984)
for (i in 1 : length(Ks)){
  K = Ks[i]
  temp = rnorm(n) #this makes it a different fold each time
  observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
  oos_residuals = array(NA, n)
  for (k in 1 : K){
    index_test = which(observation_folds == k)
    index_train = setdiff(1 : n, index_test)
    
    x_train = x[index_train]
    y_train = y[index_train]
    Xytrain = data.frame(x = x_train, y = y_train)
    x_test = x[index_test]
    y_test = y[index_test]
    
    g_mod = lm(y ~ ., Xytrain)
    y_hat_g = predict(g_mod, data.frame(x = x_test))
    oos_residuals[index_test] = y_test - y_hat_g
  }
  results[i, ] = c(sd(oos_residuals), K)
}
```

What is the variability?

```{r}
results_summary = results %>%
  group_by(K) %>%
  summarize(Kavg = mean(s_e), Kse = sd(s_e))
results_summary
```

This is a significant improvement in variability than before! (scroll up) There is greatly improved tightness for high K. Seemingly with K-fold CV, you can be more confident to use high K because it is decreasing the variance in the estimate. High K also reduces bias.

Now we plot it:

```{r}
ggplot(results) + 
  aes(x = s_e) +
  geom_density(aes(fill = factor(K)), alpha = 0.3) + 
  # xlim(0, NA) +
  xlim(3, 4.5) +
  geom_vline(data = results_summary, aes(xintercept = Kavg, color = factor(K)), size = 2) +
  geom_vline(xintercept = gen_error_true, col = "white", size = 1)
```


Admittedly, I don't know the properties of CV estimates as well as I should. Thus, there will be only procedural questions on the next exam. I do know that selecting K "optimally" for general datasets is an open question.

There is one other nice thing about having folds, you can estimate the standard error in your generalization estimate by pretending you have K iid samples and pretending the normal theory applies. For example, let's say K = 5. Instead of aggregating all residuals, we leave them separate and get K = 5 difference estimates for generalization error.

```{r}
K = 5
set.seed(1984)
temp = rnorm(n)
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)

oos_s_e_s = array(NA, K)
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 : n, index_test)
  
  x_train = x[index_train]
  y_train = y[index_train]
  Dtrain = data.frame(x = x_train, y = y_train)
  x_test = x[index_test]
  y_test = y[index_test]
  
  g_mod = lm(y ~ ., Dtrain)
  y_hat_g = predict(g_mod, data.frame(x = x_test))
  oos_s_e_s[k] = sd(y_test - y_hat_g)
}

avg_s_e = mean(oos_s_e_s)
s_s_e = sd(oos_s_e_s)
avg_s_e
s_s_e
#approx 95% CI
c(avg_s_e - 2 * s_s_e, avg_s_e + 2 * s_s_e) #no divide by sqrt(K) - that was a mistake in the notes
gen_error_true
```

Although this is technically nonsense since they're not iid samples since the training set is crossed over containing mostly the same observations, at least it's something. In the above example, we've managed to capture the true generalization error.

Coverage in this confidence interval is over D. So I wouldn't gain much insight by simulating different splits with the same K.

I believe confidence intervals for generalization error is an open problem or maybe proved that you can't find them in general situations.

Here's a real data example with the `diamonds` dataset.

```{r}
K = 5

set.seed(2000)
temp = rnorm(nrow(diamonds))
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)

all_idx = 1 : nrow(diamonds)
s_e_s = array(NA, K)
y_hat_g = array(NA, nrow(diamonds))
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 :  nrow(diamonds), index_test)
  mod = lm(price ~ ., diamonds[index_train, ])
  y_hat_g[index_test] = predict(mod, diamonds[index_test, ])
  s_e_s[k] = sd(diamonds[index_test, ]$price - y_hat_g[index_test])
}
s_e_s
mean(s_e_s)
sd(s_e_s)
#approx 95% CI
c(mean(s_e_s) - 2 * sd(s_e_s), mean(s_e_s) + 2 * sd(s_e_s))
```

Why is the $s_{s_e}$ so low? High $n$. Cross validation here was probably not even necessary.


# C++ and R

R goes back to 1995 when it was adapted from S (written in 1976 by John Chambers at Bell Labs) with minor modifications. The core of base R is written in C and Fortran. These two languages are the fastest known languages (how to measure "fastest" is a huge debate). Thus, base R is very fast. For instance the `sort` function is as fast as C/Fortran since it immediately calls compiled C/Fortran routines.

However, R code itself that you write is "interpreted" which means it is not compiled until you run it. And it has to compile on-the-fly, making it very slow. Prior to v3.4 (April, 2017) it was even slower since the code wasn't JIT compiled. All this "real CS" stuff you can learn in another class..

One notable place to observe this slowness relative to other languages is in looping. For example:

```{r}
f = function(){
  SIZE = 1e7
  v = array(NA, SIZE)
  for (i in 1 : SIZE){
    v[i] = i
  }
  v
}

v = f()
```

How long does this take?

```{r}
# install.packages("Rcpp")
pacman::p_load_current_gh("hadley/lineprof")
lineprof(f())

system.time({
  f()
})
#on my office computer for SIZE = 10,000,000:
# user  system elapsed 
# 7.13    0.20    7.35 
```

Take a simple function that computes square roots on each element:

```{r}
sqrt_vector = function(v){
  v_new = array(NA, length(v))
  for (i in 1 : length(v)){
    v_new[i] = sqrt(v[i])
  }
  v_new
}

system.time({
  sqrt_vector(v)
})
```

Does the apply function help?

```{r}
system.time({
  apply(v, MARGIN = 1, FUN = sqrt)
})
```

Strange that this takes so long? So it doesn't help... it hurts.

How much faster in C++ should this be?

Enter the `Rcpp` package - a way to compile little bits (or lotta bits) of C++ on the fly.

```{r}
pacman::p_load(Rcpp)
```


Let's write this for loop function to sqrt-ize and compile it and then save it into our namespace to be called like a regular function.

```{r}
cppFunction('
  NumericVector sqrt_vector_cpp(NumericVector v) {
    int n = v.size();
    NumericVector v_new(n);
    for (int i = 0; i < n; i++) { //indices from 0...n-1 not 1...n!
      v_new[i] = sqrt(v[i]);
    }
    return v_new;
  }
')
```

What do these two functions look like?

```{r}
sqrt_vector
sqrt_vector_cpp
```

One shows the R code and then says it is bytecode-compiled (go to an advanced CS class). The other just says we `.Call` some C++ function in a certain address and the argument to be inputted.

What is the gain in runtime?

```{r}
system.time({
  sqrt_vector_cpp(v)
})
```

WOW. 10x!!! Can't beat that with a stick...

Let's do a not-so-contrived example...

Matrix distance... Let's compute the distances of all pairs of rows in a dataset. I will try to code the R as efficiently as possible by using vector subtraction so there is only two for loops. The C++ function will have an additional loop to iterate over the features in the observations.

```{r}
#a subset of the diamonds data
X_diamonds = as.matrix(ggplot2::diamonds[1 : 3000, c("carat", "depth", "table", "x", "y", "z")])

compute_distance_matrix = function(X){
  n = nrow(X)
  D = matrix(NA, n, n)
  for (i_1 in 1 : (n - 1)){
    for (i_2 in (i_1 + 1) : n){
      D[i_1, i_2] = sqrt(sum((X[i_1, ] - X[i_2, ])^2))
    }
  }
  D
}

cppFunction('
  NumericMatrix compute_distance_matrix_cpp(NumericMatrix X) {
    int n = X.nrow();
    int p = X.ncol();
    NumericMatrix D(n, n);
    std::fill(D.begin(), D.end(), NA_REAL);

    for (int i_1 = 0; i_1 < (n - 1); i_1++){
      //Rcout << "computing for row #: " << (i_1 + 1) << "\\n";
      for (int i_2 = i_1 + 1; i_2 < n; i_2++){
        double sqd_diff = 0;
        for (int j = 0; j < p; j++){
          sqd_diff += pow(X(i_1, j) - X(i_2, j), 2); //by default the cmath library in std is loaded
        }
        D(i_1, i_2) = sqrt(sqd_diff); //by default the cmath library in std is loaded
      }
    }
    return D;
  }
')
```

```{r}
system.time({
  D = compute_distance_matrix(X_diamonds)
})
round(D[1 : 5, 1 : 5], 2)
```

Slow...

```{r}
system.time({
  D = compute_distance_matrix_cpp(X_diamonds)
})
round(D[1 : 5, 1 : 5], 2)
```

Absolutely lightning... 100x faster on my laptop than R's runtime.

Writing functions as strings that compile is annoying. It is better to have separate files. For instance...

```{r}
sourceCpp("distance_matrix.cpp")
```

Here are a list of the data structures in Rcpp: https://teuder.github.io/rcpp4everyone_en/070_data_types.html#vector-and-matrix

Another place where C++ pays the rent is recursion. Here is a quicksort implementation in R taken from 

```{r}
quicksort_R <- function(arr) {
  # Pick a number at random.
  mid <- sample(arr, 1)

  # Place-holders for left and right values.
  left <- c()
  right <- c()
  
  # Move all the smaller values to the left, bigger values to the right.
  lapply(arr[arr != mid], function(d) {
    if (d < mid) {
      left <<- c(left, d)
    }
    else {
      right <<- c(right, d)
    }
  })
  
  if (length(left) > 1) {
    left <- quicksort_R(left)
  }
  
  if (length(right) > 1) {
    right <- quicksort_R(right)
  }
  
  # Finally, return the sorted values.
  c(left, mid, right)
}
```

Let's create a random array to test these sorts on:

```{r}
n = 5e5
x = rnorm(n)
```


Let's profile the pure R sort function:

```{r}
system.time({
  x_sorted_pure_R = quicksort_R(x)
})
```

Let's profile R's `sort` function.

```{r}
system.time({
  x_sorted_base_R = sort(x)
})
```

Let's just ensure our method worked...

```{r}
pacman::p_load(testthat)
expect_equal(x_sorted_pure_R, x_sorted_base_R)
```

Basically infinitely faster. Let's make our own C++ implementation.

```{r}
sourceCpp("quicksort.cpp")
```

and profile it:

```{r}
system.time({
  x_sorted_cpp = quicksort_cpp(x)
})
```

Let's just ensure this method worked...

```{r}
pacman::p_load(testthat)
expect_equal(x_sorted_cpp, x_sorted_base_R)
```

Why is our C++ slower than `sort`. Because `sort` is also in C++ or Fortran and it's been likely optimized and reoptimized up to wazoo for decades. Also, Rcpp's data structures may be slower than base R's data structures. There may be some speed lost to translating to `NumericVector` from `double[]` or something like that.

Can you call R from Rcpp? You bet:

```{r}
cppFunction('
  NumericVector rnorm_cpp_R(int n, double mean, double sd){
      // get a pointer to R\'s rnorm() function
      Function f("rnorm");   
  
      // Next code is interpreted as rnorm(n, mean, sd)
      return f(n, Named("sd")=sd, _["mean"]=mean);
  }
')

rnorm_cpp_R(5, 1, .01)
```

A few math functions are implemented for you already:

```{r}
evalCpp('R::qnorm(0.5, 0, 1, 1, 0)')
```

Further, there are many common functions that are already wrapped for you via "Rcpp-sugar" which was the Rcpp's author's attempt to make Rcpp a whole lot easier, see [here](http://dirk.eddelbuettel.com/code/rcpp/Rcpp-sugar.pdf).

```{r}
evalCpp('rnorm(10, 100, 3)')
```

If you want blazing fast linear algebra, check out package `RcppArmadillo` which is a wrapper around Apache's Armadillo (namespace is "arma" in the code), an optimized linear algebra package in C++. Here is an example taken from [here](https://scholar.princeton.edu/sites/default/files/q-aps/files/slides_day4_am.pdf):

```{r}
pacman::p_load(RcppArmadillo, microbenchmark, testthat)

cppFunction('
  arma::mat ols_cpp(arma::mat X, arma::mat y){
    arma::mat Xt = X.t();
    return solve(Xt * X, Xt * y);
  }
', depends = "RcppArmadillo")

n = 500
D = data.frame(int = rep(1, n), x1 = rnorm(n), x2 = rnorm(n), x3 = rnorm(n), y = rnorm(n))
X = as.matrix(D[, 1 : 4])
y = as.matrix(D[, 5])

#does the function work?
expect_equal(as.numeric(ols_cpp(X, y)), as.numeric(solve(t(X) %*% X) %*% t(X) %*% y))

microbenchmark(
  R_via_lm = lm(y ~ 0 + ., data = D),
  R_matrix_multiplication = solve(t(X) %*% X) %*% t(X) %*% y,
  cpp = ols_cpp(X, y),
    times = 100
)
```

About 4x faster than R's optimized linear algebra routines. Supposedly it can go even faster if you enable parallelization within Armadillo. I couldn't get that demo to work.

Here are the places where Rcpp should be used (from https://teuder.github.io/rcpp4everyone_en/010_Rcpp_merit.html)

* Loop operations in which later iterations depend on previous iterations.
* Accessing each element of a vector/matrix.
* Recurrent function calls within loops.
* Changing the size of vectors dynamically.
* Operations that need advanced data structures and algorithms (we don't do this in this class).

## Data "Munging" with Dplyr and data.table

"Data munging", sometimes referred to as "data wrangling", is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. A data wrangler is a person who performs these transformation operations. -[Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)

Half of what a data scientist does is cleaning data, visualizing data and wrangling it. In the process you learn all about your dataset and you're on a higher level when it comes time to build prediction models.

The packages `dplyr` and `data.table` offer many conveninent functions to manipulate, clean, and otherwise wrangle data. Note: all the wrangling we're going to see *can* be done with base R (see previous notes on the `data.frame` object) but it would be *very very very very* annoying and *very very very very* slow.

I will quickly compare and contrast `dplyr` and `data.table` before you see it inside actual code.

* `dplyr` works really nicely with the piping chain as you "begin" the manipulation with the dataset and then iteratively pipe in step 1, step 2, etc until you wind up with what end product you would like. This makes `dplyr` very readable but very verbose - lots of lines of code. 
* On the flip side, `data.table` essentially wrote a new data wrangling language so it's a harder learning curve but it's very compact - very few lines of code.
* `data.table` is blazing fast and kills `dplyr` in performance and I'm pretty sure it even beats Python in performance (someone please check this). So in the era of "big data", I think this is the winner even though it is much harder to learn.
* I believe `dplyr` is more popular in the real world and thus has more cache to put on your CV. But this is constantly in flux!

For all labs and the final project, you are recommended to pick one you want to use and go with it. For the exams, I will write code in both (if need be) to not penalize / reward a student who picked one over the other.

Here is a nice [translation guide](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) between `dplyr` and `data.table`. We will be learning them in tandem. I could've split this into two units but I decided against it because (1) it is good to see the same functionality side-by-side and (2) this is really just one concept.

```{r}
pacman::p_load(tidyverse, magrittr) #tidyverse is shorthard for dplyr, ggplot2, tidyr, readr and a bunch of other packages recommended for the "full" dplyr experience. I'm using magrittr for special pipe operations later.
pacman::p_load(data.table)
```

Note that `data.table` is automatically multithreaded. Read [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/setDTthreads).

```{r}
getDTthreads()
```


We first instantiate the upgraded data.frame objects in both libraries:

```{r}
diamonds_tbl = tbl_df(diamonds) #not necessary to cast because dplyr does the conversion automatically after using any dplyr function
diamonds_dt = data.table(diamonds) #absolutely necessary
```

What happens during the data frame conversion?

```{r}
class(diamonds_tbl)
class(diamonds_dt)
```

Note how these are implemented as class extensions of R's `data.frame` as to allow for background compatibility and not break the API. They have nicer ways of showing the data:

```{r}
diamonds_tbl #run this in the console, not inside the chunk
diamonds_dt #run this in the console, not inside the chunk
```

Beginning with the simplest munging tasks, subsetting rows:

```{r}
diamonds_tbl %>% 
  slice(1 : 5)

diamonds_dt[1 : 5]
```

And subsetting columns:

```{r}
diamonds_tbl %>% 
  select(cut, carat, price) #these three only in this order

diamonds_dt[, .(cut, carat, price)]

diamonds_tbl %>% 
  select(carat, price, cut) #these three only in another order

diamonds_dt[, .(carat, price, cut)]

diamonds_tbl %>% 
  select(-x) #drop this feature

diamonds_dt[, !"x"]
#diamonds_dt[, x := NULL] #mutating function (overwrites the data frame)

diamonds_tbl %>% 
  select(-c(x, y, z)) #drop these features
diamonds_tbl %>% 
  select(-x, -y, -z) #drop these features

diamonds_dt[, !c("x", "y", "z")]
```

How about will rename a column

```{r}
diamonds_tbl %>% 
  rename(weight = carat, price_USD = price)

diamonds_dt_copy = copy(diamonds_dt)
setnames(diamonds_dt_copy, old = c("carat", "price"), new = c("weight", "price_USD")) #the `setnames` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

If you want to rearrange the columns...

```{r}
#In dplyr you pretend to select a subset and then ask for everything else:
diamonds_tbl %>% 
  select(carat, price, cut, everything()) #these three in this order first then everything else
# diamonds_tbl %>% 
#   select(-carat, everything()) #move carat last (first drop it, and then add it back in with everything)

diamonds_dt_copy = copy(diamonds_dt)
setcolorder(diamonds_dt_copy, c("carat", "price", "cut")) #as before, the `setcolorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

Sorting the rows by column(s):

```{r}
diamonds_tbl %>%
  arrange(carat) #default is ascending i.e. lowest first

diamonds_dt[order(carat)]
diamonds_dt_copy = copy(diamonds_dt)
setorder(diamonds_dt_copy, carat) #as before, the `setorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)

diamonds_tbl %>%
  arrange(desc(carat)) #switch to descending, i.e. highest first

diamonds_dt[order(-carat)] #and you can do this with `setorder` too

diamonds_tbl %>%
  arrange(desc(color), clarity, cut, desc(carat)) #multiple sorts - very powerful

diamonds_dt[order(-color, clarity, cut, -carat)] #and you can do this with `setorder` too
```

The filter method subsets the data based on conditions:

```{r}
diamonds_tbl %>%
  filter(cut == "Ideal")

diamonds_dt[cut == "Ideal"]

diamonds_tbl %>%
  filter(cut == "Ideal") %>%
  filter(depth < 65) %>%
  filter(x * y * z > 20)
diamonds_tbl %>%
  filter(cut == "Ideal" & depth < 65 & x * y * z > 20)

diamonds_dt[cut == "Ideal" & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter((cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[(cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter(cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20]
```

How about removing all rows that are the same?

```{r}
diamonds_tbl
diamonds_tbl %>%
  distinct

unique(diamonds_dt)

#nice function from data.table:
uniqueN(diamonds$carat) 
#273 < 53940 i.e. there's only a few weight measurements that are possible... let's only keep one from each unique carat value

diamonds_tbl %>%
  distinct(carat, .keep_all = TRUE) #keeps the first row for each unique weight measurement

unique(diamonds_dt, by = "carat")
```

Sampling is easy

```{r}
diamonds_tbl %>%
  sample_n(7)

diamonds_dt[sample(.N, 7)] #.N is a cool function: it is short for `nrow(dt object)`

diamonds_tbl %>%
  sample_frac(1e-3)

diamonds_dt[sample(.N, .N * 1e-3)] #.N is a cool function: it is short for `nrow(dt object)
```


Now for some real fun stuff. Let's create new features with the `mutate` function.

```{r}
diamonds_tbl %>%
  mutate(volume = x * y * z) #adds a new column keeping the old ones (this was an exam problem in a previous year)

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, volume := x * y * z]
diamonds_dt2

diamonds_tbl %>%
  mutate(price_per_carat = price / carat) %>%
  arrange(desc(price_per_carat))

diamonds_dt2[, price_per_carat := price / carat]
diamonds_dt2[order(-price_per_carat)]
rm(diamonds_dt2)
```

Or rewrite old ones.

```{r}
diamonds_tbl %>%
  mutate(cut = substr(cut, 1, 1))

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, cut := substr(cut, 1, 1)]
diamonds_dt2

diamonds_tbl %>%
  mutate(carat = factor(carat))

diamonds_dt2[, carat := factor(carat)]
diamonds_dt2
rm(diamonds_dt2)
```

Here are some more ways to create new variables. Translating to `data.table` is trivial:

```{r}
diamonds_tbl %>%
  mutate(carat = factor(ntile(carat, 5)))
diamonds_tbl %>%
  mutate(carat = percent_rank(carat))
diamonds_tbl %>%
  mutate(lag_price = lag(price)) #if this data was a time series
diamonds_tbl %>%
  mutate(cumul_price = cumsum(price)) #%>% tail
```

How about if you want to create a column and drop all other columns in the process?

```{r}
diamonds_tbl %>%
  transmute(volume = x * y * z) #adds a new column dropping the old ones

diamonds_dt[, .(volume = x * y * z)]
```

There are many ways to reshape a dataset. We will see two now and a few functions later when it becomes important. For instance: we can collapse columns together using the `unite` function from package `tidyr` (which should be loaded when you load `dplyr`). We will have a short unit on more exciting and useful reshapings later ("long" to "short" and vice-versa). As far as I know `data.table` has a less elegant... unless someone has a better idea?

```{r}
diamonds_tbl2 = diamonds_tbl %>%
  unite(dimensions, x, y, z, sep = " x ")
diamonds_tbl2

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, dimensions := paste(x, y, z, sep = " x ")] #mutating
diamonds_dt2 = diamonds_dt2[, !c("x", "y", "z")]
diamonds_dt2
```

We can reverse this operation:

```{r}
diamonds_tbl2 %>%
  separate(dimensions, c("x", "y", "z"), sep = " x ")
rm(diamonds_tbl2)

diamonds_dt2[, c("x", "y", "z") := strsplit(dimensions, "x")]
diamonds_dt2[, -"dimensions"]
rm(diamonds_dt2)
```

There are tons of other packages to do clever things. For instance, here's one that does dummies. Let's convert the color feature to dummies. Again slightly less readable or elegant in `data.table`:

```{r}
pacman::p_load(sjmisc, snakecase)
diamonds_tbl %>%
  to_dummy(color, suffix = "label") %>% #this creates all the dummies
  bind_cols(diamonds_tbl) %>% #now we have to add all the original data back in
  select(-matches("_"), everything()) %>% #this puts the dummies last
  select(-color) #finally we can drop color

cbind(
  diamonds_dt[, -"color"], 
  to_dummy(diamonds_dt[, .(color)], suffix = "label")
)
```


What if you want to create a new variable based on functions only run on subsets of the data. This is called "grouping". Grouping only makes sense for categorical variables. (If you group on a continuous variable, then chances are you'll have $n$ different groups because you'll have $n$ unique values).

For instance:

```{r}
diamonds_tbl %>%
  group_by(color)

diamonds_dt[,, by = color]
```

Nothing happened... these were directives to do things a bit differently with the addition of other logic. So after you group, you can now run operations on each group like they're their own sub-data frame. Usually, you want to *summarize* data by group. This means you take the entire sub-data frame and run one metric on it and return only those metrics (i.e. shrink $n$ rows to $L$ rows). This sounds more complicated than it is and it is where data wrangling really gets fun. 

Here are a few examples:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price))

diamonds_dt[, .(avg_price = mean(price)), by = color][order(color)] #chaining / piping [...][...][...] etc
#where did all the other rows and columns go???

diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price), sd_price = sd(price), count = n())

diamonds_dt[, .(avg_price = mean(price), sd_price = sd(price), count = .N), by = color][order(color)]

diamonds_tbl %>%
  group_by(color) %>%
  summarize(min_price = min(price), med_price = median(price), max_price = max(price))

diamonds_dt[, .(min_price = min(price), med_price = median(price), max_price = max(price)), by = color][order(color)]
```

Sometimes you want to do fancier things like actually run operations on the whole sub-data frame using `mutate`. If the function is a single metric, then that metric is then duplicated across the whole sub data frame.

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(avg_price_for_color = mean(price)) 
#creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, avg_price_for_color := mean(price), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

So that's kind of like duplicating a summary stat. Here's something more fun: actually creating a new vector:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(price_rank_within_color = dense_rank(price)) #creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, price_rank_within_color := frankv(price, ties.method = "dense"), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

What if we want to get the first row in each category?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1)

diamonds_dt[, .SD[1], by = color][order(color)]
```

The `.SD` variable is short for "sub dataframe" and it's a stand-in for the pieces of the dataframe for each color as it loops over the colors. So `.SD[1]` will be first row in the sub dataframe. The reason why the matrices come out different is that the order of the rows in data.table changes based on optimizations. We'll see some of this later. I'm also unsure why it moved the `color` column to the front.

What about first and last?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1, n())

diamonds_dt[, .SD[c(1, .N)], by = color]
```

How about the diamond with the highest price by color?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  arrange(price) %>%
  slice(n())

diamonds_dt[, .SD[which.max(price)], by = color]
```

We've seen `data.table`'s preference for mutating functions. Here is a pipe command from package `magrittr` that makes the functions mutating. 

```{r}
diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 = diamonds_tbl2 %>%
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2

diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 %<>% #pipe and overwrite (short for what's above)
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2
rm(diamonds_tbl2)
```

This is as far we will go with data wrangling right now.

Let's benchmark a few core features of both packages. To do so, let's create a dataframe that's very big:

```{r}
pacman::p_load(microbenchmark)

Nbig = 2e6
diamonds_tbl_big = diamonds_tbl %>%
  sample_n(Nbig, replace = TRUE)
diamonds_dt_big = data.table(diamonds_tbl_big) #just to make sure we have the same data
diamonds_big = data.frame(diamonds_tbl_big) #ensure that it is a base R object
```

How about we write this dataframe to the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = write.csv(diamonds_big, "diamonds_big.csv"),
  tidyverse = write_csv(diamonds_tbl_big, "diamonds_big.csv"),
  data.table = fwrite(diamonds_dt_big, "diamonds_big.csv"),
    times = 1
)
```

How about we read this dataframe from the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = read.csv("diamonds_big.csv"),
  tidyverse = read_csv("diamonds_big.csv"),
  data.table = fread("diamonds_big.csv"),
    times = 1
)
```

What about for creating new variables?

```{r}
microbenchmark(
  base_R = {diamonds_big$log_price = log(diamonds_big$price)},
  tidyverse = {diamonds_tbl_big %<>% mutate(log_price = log(price))},
  data.table = diamonds_dt_big[, log_price := log(price)],
    times = 100
)
```

About the same. How about grouping and summarizing? No easy one-liner in base R. So we just compare the two packages:


```{r}
microbenchmark(
  tidyverse = {diamonds_tbl_big %>% group_by(color) %>% summarize(avg_price = mean(price))},
  data.table = diamonds_dt_big[, .(avg_price = mean(price), by = color)],
    times = 10
)
```

How about sorting?

```{r}
microbenchmark(
  base_R = diamonds_big[order(diamonds_big$price), ],
  tidyverse = {diamonds_tbl_big %>% arrange(price)},
  data.table = diamonds_dt_big[order(price)],
    times = 10
)
```
How about filtering?

```{r}
microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 10
)
```

Let's do this again but first "key" the price column which is what you would do if you are doing lots of searches.

```{r}
setkey(diamonds_dt_big, price)

microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 30
)
```

We still have to learn how to reshape tables and join multiple tables together. We will do that later.
