---
title: "Practice Lecture 19 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 14, 2021"
---

# Model Selection and Three Data Splits

This unit is split into three use cases (1) Selection among M explicit models (2) Hyperparameter Selection within one algorithm (3) Stepwise Model Construction

## Use Case (I) Selecting one of M Explicit Models

We have now covered non-linearities (e.g. polynomial terms) and interactions. A new complication now clearly emerges. If I have $p$ predictors, there are many linear least squares models I can build (considering non-linear least squares models makes the space of models even larger!!)

For instance, here are a bunch of models and their in-sample performance:

```{r}
pacman::p_load(ggplot2)
all_model_formulas = list( #note: need these as strings for later...
  "price ~ carat + depth",
  "price ~ carat + depth + color + x + y + z",
  "price ~ .",
  "price ~ . * ."
)
mods = lapply(all_model_formulas, lm, diamonds)
lapply(mods, function(mod){summary(mod)$r.squared})
```

Obviously the in-sample R^2's are increasing due to the complexity, but which model is "best"?

This is one of the most fundamental problems in statistics, and possibly all of science! 

In class, we discussed validation via dividing $\mathbb{D}$ into (a) a training set and a (b) testing set. Now, we will further divide the training set into (a) a sub-training set and a (b) selection set and we still have the (c) test set. 

The total training set together will fit a model and testing will estimate future performance. But within the total training set, we'll use an elaborate algorithim: we'll fit many models and take the best one. That's the "master algorithm".

We'll make the selection set and the test set the same size but we don't have to. First split up the data:

```{r}
n = nrow(diamonds)
K = 5
set.seed(1984)
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

#make sure we did this right:
pacman::p_load(testthat)
expect_equal(1 : n, sort(c(train_indices, select_indices, test_indices)))

diamonds_train = diamonds[train_indices, ]
diamonds_select = diamonds[select_indices, ]
diamonds_test = diamonds[test_indices, ]

rm(test_indices, select_indices, train_indices)
```

Now, fit all models and select the best one:

```{r}
mods = lapply(all_model_formulas, lm, diamonds_train)
```

Now predict on the selection set and look at the oos $s_e$, and select the "best" model

```{r}
yhat_selects = lapply(mods, function(mod){predict(mod, diamonds_select)})
y_select = diamonds_select$price #the true prices

s_e_s = lapply(yhat_selects, function(yhat_select){sd(yhat_select - y_select)})
s_e_s
#find the minimum
which.min(s_e_s)
```

Which are overfit? Which are underfit? Were these models "poor choices"?

Can we go back and fit some more models? 

Yes - as long as we don't open the "lockbox" of the test set. Let's look at one more model. An expansion of the best of the previous 4 models now with a couple interactions we are convinced are real plus a couple of non-linear terms:

```{r}
all_model_formulas[[5]] = "price ~ . + carat * color + carat * depth + I(carat^2) + I(depth^2)"
mods[[5]] = lm(all_model_formulas[[5]], diamonds_train) 

yhat_selects[[5]] = predict(mods[[5]], diamonds_select)

s_e_s[[5]] = sd(yhat_selects[[5]] - y_select)
s_e_s
#find the minimum
which.min(s_e_s)
```

We can go further and fit more and more models but we should always be careful that we don't fit too many as we may optimize to the selection set. Here, we are lucky since the selection set is large (~11,000 observations) so this is not too much of a fear.

But you can see the problem - how can we build a good model??

The answer to this is non-parametric regression / machine learning. But first, we will cover two other important topics before we get there.

Let us return and complete the exercise by now declaring we are done modeling and we are going to ship model 5. Let us get a conservative estimate of its performance:

```{r}
mod5_for_test = lm(all_model_formulas[[5]], rbind(diamonds_train, diamonds_select))
yhat_test_mod5 = predict(mod5_for_test, diamonds_test)
y_test = diamonds_test$price #the true prices
sd(yhat_test_mod5 - y_test)
```

About the same as the selection estimate --- we did not overfit too much to the selection set.

At this point the lockbox is open and we can never return (if we are honest, of course - many people in this business lie so beware).

Now we can build production model 4 with all data to ship:

```{r}
mod_final = lm(all_model_formulas[[5]], diamonds)
```

No evaluation is done on the final model. It is only used to predict future diamonds' prices.

Two improvements using CV to the above:

* To reduce variance in the selection process, you make a CV of the selection set. 
* To reduce variance in the testing process, you make an outer CV of the test set. This is a lot more coding!

### Using the MLR package for Model selection

Can we use MLR for Linear Model Selection?

Yes, but it is not as nice as I would've liked but it sure beats doing it yourself. I've figured it out by creating my own custom code. Warning: this is the old version of mlr (v2) as I couldn't figure it out in the new version of mlr (mlr3)!!

```{r}
pacman::p_load(mlr)
```


First, we create the task:

```{r}
modeling_task = makeRegrTask(data = diamonds, target = "price") #instantiate the task
```

Now we create a new learner which is a wrapper for the linear model with a custom formula. We need to specify learning parameters, a training function (build g) and a predict function. Then we need to add theese functions to the namespace in a way mlr understands.

```{r}
makeRLearner.regr.custom_ols = function() {
  makeRLearnerRegr(
    cl = "regr.custom_ols",
    package = "base",
    par.set = makeParamSet(
      makeDiscreteLearnerParam(id = "formula", default = all_model_formulas[[1]], values = all_model_formulas)
    ),
    properties = c("numerics", "factors", "ordered"),
    name = "Custom OLS with a Formula",
    short.name = "custom_ols"
  )
}

trainLearner.regr.custom_ols = function(.learner, .task, .subset, .weights = NULL, ...){
  lm(list(...)$formula, data = getTaskData(.task, .subset))
}

predictLearner.regr.custom_ols = function (.learner, .model, .newdata, ...){
    predict(.model$learner.model, newdata = .newdata, ...)
}

registerS3method("makeRLearner", "regr.custom_ols", makeRLearner.regr.custom_ols)
registerS3method("trainLearner", "regr.custom_ols", trainLearner.regr.custom_ols)
registerS3method("predictLearner", "regr.custom_ols", predictLearner.regr.custom_ols)
```

Now we create the "inner loop". Here, we cross validate over the different models. We do this by specifying a "tune wrapper" since technically each formula is considered a tuning paramter / hyperparameter the linear model on this task.

```{r}
Kinner = 3
all_model_param_set = makeParamSet(
  makeDiscreteParam(id = "formula", default = all_model_formulas[[1]], values = all_model_formulas)
)
inner_loop = makeResampleDesc("CV", iters = Kinner)
lrn = makeTuneWrapper("regr.custom_ols", #instantiate the OLS learner algorithm
        resampling = inner_loop, 
        par.set = all_model_param_set, 
        control = makeTuneControlGrid(), 
        measures = list(rmse))
```

We now create the outer loop and execute:

```{r}
Kouter = 5
outer_loop = makeResampleDesc("CV", iters = Kouter)
r = resample(lrn, modeling_task, resampling = outer_loop, extract = getTuneResult, measures = list(rmse))
```

Now we look at the results a bunch of different ways:

```{r}
r #overall estimate of oos error of the whole procedure if it were used on all of $\mathbb{D}$
print(getNestedTuneResultsOptPathDf(r)) #results of each inner validation over all outer iterations
r$extract #"winning" model for each outer iteration
```

See https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html? for info on inner and outer loop CV.

# Use Case (II) Forward Stepwise Model Construction

There are many types of such stepwise models. Here we will look at Forward Stepwise Linear models. "Forward" meaning we start with a low complexity model and end with a high complexity model, "Stepwise" meaning we do so iteratively which each step consisting of one additional degree of freedom i.e. one incremental increase in complexity and "Linear" meaning that the model is linear. By default we use OLS.

We will be using the diamonds data again as an example. Let's make sure we have unordered factors to avoid issues later:

```{r}
pacman::p_load(tidyverse, magrittr)
diamonds %<>%
  mutate(cut = factor(as.character(cut)), color = factor(as.character(color)), clarity = factor(as.character(clarity)))
```

What we're doing will be highly computational, so let's take a random sample of the dimaonds in $\mathbb{D}$ for training and testing:

```{r}
Nsamp = 1300
set.seed(1984)
train_indices = sample(1 : nrow(diamonds), Nsamp)
diamonds_train = diamonds[train_indices, ]
test_indices = sample(setdiff(1 : nrow(diamonds), train_indices), Nsamp)
diamonds_test = diamonds[test_indices, ]
```

Let's built a model with all second-order interactions e.g. all things that look like depth x table x clarity or depth^2 x color or depth^3.

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
coef(mod)[1000 : 1100]
```

For features that are non-binary, it's p_non_binary^3 features. Binary features are more complicated because its each level in feature A times each level in feature B. There are no squared or cube terms for binary features (since they're all the same i.e. ${0,1}^d = {0,1}$).

Remember we lkely overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try it on the another 10,000 we didn't see...

```{r}
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ --- why? What should that say about the relationship between $s_e$ and $s_y$?

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck! This means you can do better using the null model (average of y) instead of this model.

So let us employ stepwise to get a "good" model. We need our basis predictors to start with. How about the linear components of `. * . * .` --- there's nothing intrinsically wrong with that - it's probably a good basis for $f(x)$. Let's create the model matrix for both train and test:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)
p_plus_one

Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r}
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (j_try %in% predictor_by_iteration){
      next 
    }
    Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
    all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
  }
  j_star = which.min(all_ses)
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_test = Xmm_test[, predictor_by_iteration, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_test - y_hat_test)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i = ", i, "in sample: se = ", all_ses[j_star], "oos_se", oos_se, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
}
```

Now let's look at our complexity curve:

```{r}
load("stepwise.RData") #I actually ran the simulation all night and found...
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylim(0, max(c(simulation_results$in_sample_ses_by_iteration, simulation_results$oos_ses_by_iteration)))
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. If we want an exact procedure, we'd probably fit a separate smoothing regression to the oos results and analytically find the arg-minimum, $j^*$. That number will then be fed into the model matrix to create the right feature set and the final model will be produced with all the data. Or we can just stop as soon as oos error goes up. You can also obviously do CV within each iterations to stabilize this further (lab exercise).

```{r}
p_opt = which.min(oos_ses_by_iteration)
colnames(Xmm_train)[predictor_by_iteration[1 : p_opt]]
```

What is the "optimal model"?



Can we honestly assess future performance now? No... why? Our test set was really our select set and we don't have a third test set (lab exercise). Inner and outer folding can be done too as we discussed.




## Use Case (III) Hyperparameter Selection

Now we use `mlr3`, the latest version (sorry about before!)

```{r}
pacman::p_unload(mlr)
pacman::p_load(mlr3)
```

We load the breast cancer dataset from earlier in the class.

```{r}
cancer = MASS::biopsy %>%
  select(-ID) %>% #drop the useless ID column
  na.omit #drop all rows that are missing
task = TaskClassif$new(id = "cancer", backend = cancer, target = "class")
```

We now create the learner. By default, SVM is not included, so we need to load an extension package. We ensure the SVM is linear like the one we studied in class (we don't have time to do nonlinear SVM's but it is basically an attempt to make the candidate space richer).

```{r}
pacman::p_load(mlr3learners)
learner = lrn("classif.svm")
learner$param_set$values = list(kernel = "linear")
learner$param_set$values$type = "C-classification" #unsure why I need this...
```

Now we create the inner loop where we try many different values of the hyperparameter via a grid search. This grid search functionality has been further decomped in the new mlr3 package into a subpackage called `mlr3tuning`. 

```{r}
pacman::p_load(mlr3tuning)
resampling = rsmp("holdout")
measure = msr("classif.ce") #misclassification error
search_space = ps(cost = p_dbl(lower = 0.0001, upper = 1))

Kinner = 5
terminator = trm("evals", n_evals = Kinner)
tuner = tnr("grid_search", resolution = 30)
at = AutoTuner$new(learner, resampling, measure, terminator, tuner, search_space)
```

Now we create the outer loop and execute

```{r}
Kouter = 3
resampling_outer = rsmp("cv", folds = Kouter)
rr = resample(task = task, learner = at, resampling = resampling_outer)
```

Now we look at the results a bunch of different ways:

```{r}
rr$score()
rr$aggregate()
rr$prediction()$confusion
```

