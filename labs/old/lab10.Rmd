---
title: "Lab 10"
author: "Your Name Here"
output: pdf_document
date: "11:59PM May 11, 2020"
---




Fit a tree to this data. Try to use `YARF` if you have it. If not, use the package `rpart`. Below is a guide to installing `YARF` and ensuring it works.

First, ensure you have the Java JDK installed. The JDK is NOT the JRE. The former allows you to compile Java programs and the latter allows you only to run Java programs. Then insure that `rJava` is installed and working. In other words, the following should work and give the same output from practice lecture 12. If it doesn't, try the code that is commented out to reinstall. Google errors. Frustration in libraries and platforms not working on your computer is unfortunately part of computer science and thus part of data science.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(rJava)
#if that doesn't work, use:
# install.packages("rJava", type = "source")
# library(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue") #java_double.intValue();
#call a static method
J("java/lang/String", "valueOf", java_double) #String.valueOf(java_double);
J("java/lang/String", "valueOf", x) #some sort of alphanumeric code for the pointer address
```

It is important to have rJava working on your computer as a fair number of R packages really do make use of it. It's a good thing to have in your toolbox in general.

Now ensure that YARF is installed properly:


```{r}
# pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
# pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev")
pacman::p_load(YARF)
```

If that printed out "YARF can now make use of [n] cores", you are in business.

Now create a training-test split and make the tree model and provide oos performance metrics: create a confusion table and compute FDR and FOR.

```{r}
#TO-DO
```


We are done with this unit. 

Let's take a look at the simulated sine curve data from practice lecture 12. Below is the code for the data generating process:

```{r}
rm(list = ls())
n = 500
sigma = 0.3
x_min = 0
x_max = 10
x = runif(n, x_min, x_max)
f_x = function(x){sin(x)}
y = f_x(x) + rnorm(n, 0, sigma)
```

Plot an example dataset of size 500:

```{r}
#TO-DO
```

Locate the optimal node size hyperparameter for the regression tree model.

```{r}
#TO-DO
```

Plot the regression tree model with the optimal node size.

```{r}
#TO-DO
```

Provide the bias-variance decomposition of this DGP fit with this model. It is a lot of code, but it is in the practice lectures.

```{r}
#TO-DO
```

Load the boston housing data. Leave 25% of the observations oos for honest validation. 

```{r}
library(MASS)
data(Boston)
Boston = Boston[sample(1:nrow(Boston)), ]
Boston_train = Boston[1 : 380, ]
Boston_test = Boston[381 : nrow(Boston), ]
```

Fit a linear model with all first-order interactions and provide std err of residuals in the test set.

```{r}
mod = lm(medv ~ .*., Boston_train)
mod
yhat = predict(mod, Boston_test)
sd(Boston_test$medv - yhat)
```

Bag this algorithm with $M = 1000$ and provide std err of residuals in the test set. 

```{r}
M = 1000
for (m in 1:M) {
  
}
```

What is your gain over the unbagged model? Why is there a gain?

#TO-DO












